{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang di Penambangan Data \u00b6 Nama : Anggrahito Cahyo Dwi Prasetyaningtyas NIM : 180411100019 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab, S.Si., M.Kom Jurusan : Teknik Informatika Alamat : Desa Gempolkurung, Kec. Menganti, Kab. Gresik","title":"Index"},{"location":"#selamat-datang-di-penambangan-data","text":"Nama : Anggrahito Cahyo Dwi Prasetyaningtyas NIM : 180411100019 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab, S.Si., M.Kom Jurusan : Teknik Informatika Alamat : Desa Gempolkurung, Kec. Menganti, Kab. Gresik","title":"Selamat Datang di Penambangan Data"},{"location":"Mengukur Jarak/","text":"Mengukur Jarak Data \u00b6 A. Mengukur Jarak Tipe Numerik \u00b6 Salah satu tantangan era saat ini adalah datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Algoritma seperti Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan. Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaitu v1, v2 menyatakandua vektor yang menyatakan v1=x1, x2,...,xn, v2=y1,y2,...,yn, dimana xi, yi disebut attribut. Ada beberapa ukuran similaritas data ukuran jarak, diantaranya : 1. Minkowski Distance \u00b6 Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ dimana m adalah bilangan riel positif dan xi dan yi adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. 2. Manhattan Distance \u00b6 Manhattan distance adalah kasus khusus dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. Bila ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ 3. Euclidean Distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkinan memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. 4. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adalah versi modikfikasi dari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x, y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ 5. Weighted Euclidean Distance \u00b6 Jika berdasarkan tingkatan penting dari masing-masing atribut ditentukan, maka Weighted Euclidean Distance adalah modifikisasi lain dari jarak Euclidean Distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana wi adalah bobot yang diberikan pada atribut ke i. 6. Chord Distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana \u2225x\u22252 adalah $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$ 7. Mahalanobis Distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ dimana S adalah matrik covariance data. 8. Cosine Measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan $$ Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } $$ dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn) didefinisikan dengan $$ |y|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } $$ 9. Pearson Correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara dua bentuk pola expresi gen. Pearson correlation didefinisikan dengan $$ Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } $$ The Pearson Correlation kelemahannya adalah sensitif terhadap outlier B. Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Atribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? Satu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 dimana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, dimana p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, dimana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i dan j dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient C. Mengukur Jarak Tipe categorical \u00b6 1. Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana n adalah banyaknya atribut, ai(x) dan ai(y) adalah nilai atribut ke i yaitu Ai dari masing masing objek x dan y, \u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. 2. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana C adalah banyaknya kelas, P(c|ai(x)) adalah probabilitas bersyarat dimana kelas x adalah c dari atribut Ai, yang memiliki nilai ai(x), P(c|ai(y)) adalah probabilitas bersyarat dimana kelas y adalah c dengan atribut Ai memiliki nilai ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ dimana probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes 3. Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ D. Mengukur Jarak Tipe Ordinal \u00b6 Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f yang memiliki Mf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f untuk objek ke-i adalah xif, dan ff memiliki Mf status urutan , mewakili peringkat 1,..,Mf Ganti setiap xif dengan peringkatnya, rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi E. Menghitung Jarak Tipe Campuran \u00b6 Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0]. Misalkan data berisi atribut p tipe campuran. Ketidaksamaan (disimilarity) antara objek i dan j dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Mengukur Jarak"},{"location":"Mengukur Jarak/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak/#a-mengukur-jarak-tipe-numerik","text":"Salah satu tantangan era saat ini adalah datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Algoritma seperti Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan. Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaitu v1, v2 menyatakandua vektor yang menyatakan v1=x1, x2,...,xn, v2=y1,y2,...,yn, dimana xi, yi disebut attribut. Ada beberapa ukuran similaritas data ukuran jarak, diantaranya :","title":"A. Mengukur Jarak Tipe Numerik"},{"location":"Mengukur Jarak/#1-minkowski-distance","text":"Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ dimana m adalah bilangan riel positif dan xi dan yi adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"1. Minkowski Distance"},{"location":"Mengukur Jarak/#2-manhattan-distance","text":"Manhattan distance adalah kasus khusus dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. Bila ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$","title":"2. Manhattan Distance"},{"location":"Mengukur Jarak/#3-euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkinan memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"3. Euclidean Distance"},{"location":"Mengukur Jarak/#4-average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adalah versi modikfikasi dari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x, y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$","title":"4. Average Distance"},{"location":"Mengukur Jarak/#5-weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing-masing atribut ditentukan, maka Weighted Euclidean Distance adalah modifikisasi lain dari jarak Euclidean Distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana wi adalah bobot yang diberikan pada atribut ke i.","title":"5. Weighted Euclidean Distance"},{"location":"Mengukur Jarak/#6-chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana \u2225x\u22252 adalah $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$","title":"6. Chord Distance"},{"location":"Mengukur Jarak/#7-mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ dimana S adalah matrik covariance data.","title":"7. Mahalanobis Distance"},{"location":"Mengukur Jarak/#8-cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan $$ Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } $$ dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn) didefinisikan dengan $$ |y|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } $$","title":"8. Cosine Measure"},{"location":"Mengukur Jarak/#9-pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara dua bentuk pola expresi gen. Pearson correlation didefinisikan dengan $$ Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } $$ The Pearson Correlation kelemahannya adalah sensitif terhadap outlier","title":"9. Pearson Correlation"},{"location":"Mengukur Jarak/#b-mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Atribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? Satu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 dimana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, dimana p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, dimana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i dan j dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient","title":"B. Mengukur Jarak Atribut Binary"},{"location":"Mengukur Jarak/#c-mengukur-jarak-tipe-categorical","text":"","title":"C. Mengukur Jarak Tipe categorical"},{"location":"Mengukur Jarak/#1-overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana n adalah banyaknya atribut, ai(x) dan ai(y) adalah nilai atribut ke i yaitu Ai dari masing masing objek x dan y, \u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"1. Overlay Metric"},{"location":"Mengukur Jarak/#2-value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana C adalah banyaknya kelas, P(c|ai(x)) adalah probabilitas bersyarat dimana kelas x adalah c dari atribut Ai, yang memiliki nilai ai(x), P(c|ai(y)) adalah probabilitas bersyarat dimana kelas y adalah c dengan atribut Ai memiliki nilai ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ dimana probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes","title":"2. Value Difference Metric (VDM)"},{"location":"Mengukur Jarak/#3-minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"3. Minimum Risk Metric (MRM)"},{"location":"Mengukur Jarak/#d-mengukur-jarak-tipe-ordinal","text":"Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f yang memiliki Mf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f untuk objek ke-i adalah xif, dan ff memiliki Mf status urutan , mewakili peringkat 1,..,Mf Ganti setiap xif dengan peringkatnya, rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi","title":"D. Mengukur Jarak Tipe Ordinal"},{"location":"Mengukur Jarak/#e-menghitung-jarak-tipe-campuran","text":"Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0]. Misalkan data berisi atribut p tipe campuran. Ketidaksamaan (disimilarity) antara objek i dan j dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"E. Menghitung Jarak Tipe Campuran"},{"location":"Missing V/","text":"Missing Values \u00b6 Pengertian \u00b6 Missing value merupakan tidak tersedianya informasi pada suatu objek (kasus) yang terjadi karena informasi tersebut tidak diberikan, sulit dicari, atau memang tidak ada. Missing value sebernarnya tidak menjadi masalah bagi keseluruhan data, apalagi jika jumlahnya sedikit, misal hanya 1 % dari seluruh data. Namun jika persentase data yang hilang tersebut cukup besar, maka perlu dilakukan pengujian apakah data yang mengandung banyak missing tersebut masih layak diproses lebih lanjut ataukah tidak. Missing Values using KNN \u00b6 KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan. Kalibrasi Parameter KNN \u00b6 Jumlah tetangga yang harus dicari \u00b6 Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan. Metode agregasi untuk digunakan \u00b6 Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal Normalisasi data \u00b6 Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan. Atribut numerik jarak \u00b6 Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...). Atribut kategorikal jarak \u00b6 tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya Contoh Script Missing Values \u00b6 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0 Sumber : https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/","title":"Missing Values"},{"location":"Missing V/#missing-values","text":"","title":"Missing Values"},{"location":"Missing V/#pengertian","text":"Missing value merupakan tidak tersedianya informasi pada suatu objek (kasus) yang terjadi karena informasi tersebut tidak diberikan, sulit dicari, atau memang tidak ada. Missing value sebernarnya tidak menjadi masalah bagi keseluruhan data, apalagi jika jumlahnya sedikit, misal hanya 1 % dari seluruh data. Namun jika persentase data yang hilang tersebut cukup besar, maka perlu dilakukan pengujian apakah data yang mengandung banyak missing tersebut masih layak diproses lebih lanjut ataukah tidak.","title":"Pengertian"},{"location":"Missing V/#missing-values-using-knn","text":"KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan.","title":"Missing Values using KNN"},{"location":"Missing V/#kalibrasi-parameter-knn","text":"","title":"Kalibrasi Parameter KNN"},{"location":"Missing V/#jumlah-tetangga-yang-harus-dicari","text":"Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan.","title":"Jumlah tetangga yang harus dicari"},{"location":"Missing V/#metode-agregasi-untuk-digunakan","text":"Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal","title":"Metode agregasi untuk digunakan"},{"location":"Missing V/#normalisasi-data","text":"Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan.","title":"Normalisasi data"},{"location":"Missing V/#atribut-numerik-jarak","text":"Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...).","title":"Atribut numerik jarak"},{"location":"Missing V/#atribut-kategorikal-jarak","text":"tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya","title":"Atribut kategorikal jarak"},{"location":"Missing V/#contoh-script-missing-values","text":"# importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0 Sumber : https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/","title":"Contoh Script Missing Values"},{"location":"Pohon Keputusan/","text":"Pohon Keputusan \u00b6 Pengertian \u00b6 Pohon Keputusan (Decision Tree) adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Pohon Keputusan merupakan salah satu metode klasifikasi yang paling populer karena mudah untuk diinterpretasi oleh manusia. Pohon yang dalam analisis pemecahan masalah pengambilan keputusan adalah pemetaan mengenai alternatif-alternatif pemecahan masalah yang dapat diambil dari masalah tersebut. Konsep Pohon Keputusan \u00b6 Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Data dalam pohon keputusan biasanya dinyatakan dalam bentuk tabel dengan atribut dan record. Atribut menyatakan suatu parameter yang dibuat sebagai kriteria dalam pembentukan tree. Pohon keputusan merupakan himpunan aturan IF\u2026THEN. Setiap path dalam tree dihubungkan dengan sebuah aturan, di mana premis terdiri atas sekumpulan node-node yang ditemui, dan kesimpulan dari aturam terdiri atas kelas yang terhubung dengan leaf dari path. Bagian awal dari pohon keputusan ini adalah titik akar (root), sedangkan setiap cabang dari pohon keputusan merupakan pembagian berdasarkan hasil uji, dan titik akhir (leaf) merupakan pembagian kelas yang dihasilkan. Manfaat Pohon Keputusan \u00b6 Proses pada pohon keputusan adalah mengubah bentuk data (tabel) menjadi model pohon, mengubah model pohon menjadi rule, dan menyederhanakan rule. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk mem-break down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan mempunyai 3 tipe simpul \u00b6 Simpul akar, dimana tidak memiliki cabang yang masuk dan memiliki cabang lebih dari satu, terkadang tidak memiliki cabang sama sekali. Simpul ini biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. Simpul internal, dimana hanya memiliki 1 cabang yang masuk, dan memiliki lebih dari 1 cabang yang keluar. Simpul daun, atau simpul akhir dimana hanya memiliki 1 cabang yang masuk, dan tidak memiliki cabang sama sekali dan menandai bahwa simpul tersebut merupakan label kelas. Kelebihan Pohon Keputusan \u00b6 Tidak memerlukan biaya yang mahal saat membangun algoritma. Mudah untuk diinterpetasikan. Mudah mengintegrasikan dengan sistem basis data. Memiliki nilai ketelitian yang lebih baik. Dapat menemukan hubungan tak terduga dan suatu data. Dapat menggunakan data pasti/mutlak atau data kontinu. Mengakomodasi data yang hilang. Kekurangan pohon keputusan \u00b6 Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain. Cara Membuat Decision Tree \u00b6 Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen ( kolom , data ): kelas = [] for i in range ( len ( data )): if data . values . tolist ()[ i ][ kolom ] not in kelas : kelas . append ( data . values . tolist ()[ i ][ kolom ]) return kelas kelas = banyak_elemen ( df . shape [ 1 ] - 1 , df ) outlook = banyak_elemen ( df . shape [ 1 ] - 5 , df ) temp = banyak_elemen ( df . shape [ 1 ] - 4 , df ) humidity = banyak_elemen ( df . shape [ 1 ] - 3 , df ) windy = banyak_elemen ( df . shape [ 1 ] - 2 , df ) print ( kelas , outlook , temp , humidity , windy ) [ 'no' , 'yes' ] [ 'sunny' , 'overcast' , 'rainy' ] [ 'hot' , 'mild' , 'cool' ] [ 'high' , 'normal' ] [ False , True ] Fungsi count Kelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas ( kelas , kolomKelas , data ): hasil = [] for x in range ( len ( kelas )): hasil . append ( 0 ) for i in range ( len ( data )): for j in range ( len ( kelas )): if data . values . tolist ()[ i ][ kolomKelas ] == kelas [ j ]: hasil [ j ] += 1 return hasil pKelas = countvKelas ( kelas , df . shape [ 1 ] - 1 , df ) pKelas [ 5 , 9 ] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy ( T ): hasil = 0 jumlah = 0 for y in T : jumlah += y for z in range ( len ( T )): if jumlah != 0 : T [ z ] = T [ z ] / jumlah for i in T : if i != 0 : hasil -= i * math . log ( i , 2 ) return hasil def e_list ( atribut , n ): temp = [] tx = t_list ( atribut , n ) for i in range ( len ( atribut )): ent = entropy ( tx [ i ]) temp . append ( ent ) return temp tOutlook = t_list ( outlook , 5 ) tTemp = t_list ( temp , 4 ) tHum = t_list ( humidity , 3 ) tWin = t_list ( windy , 2 ) print ( \"Sunny, Overcast, Rainy\" , eOutlook ) print ( \"Hot, Mild, Cold\" , eTemp ) print ( \"High, Normal\" , eHum ) print ( \"False, True\" , eWin ) Sunny , Overcast , Rainy [ 0.9709505944546686 , 0.0 , 0.9709505944546686 ] Hot , Mild , Cold [ 1.0 , 0.9182958340544896 , 0.8112781244591328 ] High , Normal [ 0.9852281360342516 , 0.5916727785823275 ] False , True [ 0.8112781244591328 , 1.0 ] berikut contoh data yang akan di rubah menjadi decision tree 0 1 2 3 4 0 CUSTOMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUXURY EXTRA LARGE C1 15 15 F LUXURY SMALL C1 16 16 F LUXURY SMALL C1 17 17 F LUXURY MEDIUM C1 18 18 F LUXURY MEDIUM C1 19 19 F LUXURY MEDIUM C1 20 20 F LUXURY LARGE C1 pertama mencari entropy(s) dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"Pohon Keputusan"},{"location":"Pohon Keputusan/#pohon-keputusan","text":"","title":"Pohon Keputusan"},{"location":"Pohon Keputusan/#pengertian","text":"Pohon Keputusan (Decision Tree) adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Pohon Keputusan merupakan salah satu metode klasifikasi yang paling populer karena mudah untuk diinterpretasi oleh manusia. Pohon yang dalam analisis pemecahan masalah pengambilan keputusan adalah pemetaan mengenai alternatif-alternatif pemecahan masalah yang dapat diambil dari masalah tersebut.","title":"Pengertian"},{"location":"Pohon Keputusan/#konsep-pohon-keputusan","text":"Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Data dalam pohon keputusan biasanya dinyatakan dalam bentuk tabel dengan atribut dan record. Atribut menyatakan suatu parameter yang dibuat sebagai kriteria dalam pembentukan tree. Pohon keputusan merupakan himpunan aturan IF\u2026THEN. Setiap path dalam tree dihubungkan dengan sebuah aturan, di mana premis terdiri atas sekumpulan node-node yang ditemui, dan kesimpulan dari aturam terdiri atas kelas yang terhubung dengan leaf dari path. Bagian awal dari pohon keputusan ini adalah titik akar (root), sedangkan setiap cabang dari pohon keputusan merupakan pembagian berdasarkan hasil uji, dan titik akhir (leaf) merupakan pembagian kelas yang dihasilkan.","title":"Konsep Pohon Keputusan"},{"location":"Pohon Keputusan/#manfaat-pohon-keputusan","text":"Proses pada pohon keputusan adalah mengubah bentuk data (tabel) menjadi model pohon, mengubah model pohon menjadi rule, dan menyederhanakan rule. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk mem-break down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target.","title":"Manfaat Pohon Keputusan"},{"location":"Pohon Keputusan/#pohon-keputusan-mempunyai-3-tipe-simpul","text":"Simpul akar, dimana tidak memiliki cabang yang masuk dan memiliki cabang lebih dari satu, terkadang tidak memiliki cabang sama sekali. Simpul ini biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. Simpul internal, dimana hanya memiliki 1 cabang yang masuk, dan memiliki lebih dari 1 cabang yang keluar. Simpul daun, atau simpul akhir dimana hanya memiliki 1 cabang yang masuk, dan tidak memiliki cabang sama sekali dan menandai bahwa simpul tersebut merupakan label kelas.","title":"Pohon keputusan mempunyai 3 tipe simpul"},{"location":"Pohon Keputusan/#kelebihan-pohon-keputusan","text":"Tidak memerlukan biaya yang mahal saat membangun algoritma. Mudah untuk diinterpetasikan. Mudah mengintegrasikan dengan sistem basis data. Memiliki nilai ketelitian yang lebih baik. Dapat menemukan hubungan tak terduga dan suatu data. Dapat menggunakan data pasti/mutlak atau data kontinu. Mengakomodasi data yang hilang.","title":"Kelebihan Pohon Keputusan"},{"location":"Pohon Keputusan/#kekurangan-pohon-keputusan","text":"Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain.","title":"Kekurangan pohon keputusan"},{"location":"Pohon Keputusan/#cara-membuat-decision-tree","text":"Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen ( kolom , data ): kelas = [] for i in range ( len ( data )): if data . values . tolist ()[ i ][ kolom ] not in kelas : kelas . append ( data . values . tolist ()[ i ][ kolom ]) return kelas kelas = banyak_elemen ( df . shape [ 1 ] - 1 , df ) outlook = banyak_elemen ( df . shape [ 1 ] - 5 , df ) temp = banyak_elemen ( df . shape [ 1 ] - 4 , df ) humidity = banyak_elemen ( df . shape [ 1 ] - 3 , df ) windy = banyak_elemen ( df . shape [ 1 ] - 2 , df ) print ( kelas , outlook , temp , humidity , windy ) [ 'no' , 'yes' ] [ 'sunny' , 'overcast' , 'rainy' ] [ 'hot' , 'mild' , 'cool' ] [ 'high' , 'normal' ] [ False , True ] Fungsi count Kelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas ( kelas , kolomKelas , data ): hasil = [] for x in range ( len ( kelas )): hasil . append ( 0 ) for i in range ( len ( data )): for j in range ( len ( kelas )): if data . values . tolist ()[ i ][ kolomKelas ] == kelas [ j ]: hasil [ j ] += 1 return hasil pKelas = countvKelas ( kelas , df . shape [ 1 ] - 1 , df ) pKelas [ 5 , 9 ] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy ( T ): hasil = 0 jumlah = 0 for y in T : jumlah += y for z in range ( len ( T )): if jumlah != 0 : T [ z ] = T [ z ] / jumlah for i in T : if i != 0 : hasil -= i * math . log ( i , 2 ) return hasil def e_list ( atribut , n ): temp = [] tx = t_list ( atribut , n ) for i in range ( len ( atribut )): ent = entropy ( tx [ i ]) temp . append ( ent ) return temp tOutlook = t_list ( outlook , 5 ) tTemp = t_list ( temp , 4 ) tHum = t_list ( humidity , 3 ) tWin = t_list ( windy , 2 ) print ( \"Sunny, Overcast, Rainy\" , eOutlook ) print ( \"Hot, Mild, Cold\" , eTemp ) print ( \"High, Normal\" , eHum ) print ( \"False, True\" , eWin ) Sunny , Overcast , Rainy [ 0.9709505944546686 , 0.0 , 0.9709505944546686 ] Hot , Mild , Cold [ 1.0 , 0.9182958340544896 , 0.8112781244591328 ] High , Normal [ 0.9852281360342516 , 0.5916727785823275 ] False , True [ 0.8112781244591328 , 1.0 ] berikut contoh data yang akan di rubah menjadi decision tree 0 1 2 3 4 0 CUSTOMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUXURY EXTRA LARGE C1 15 15 F LUXURY SMALL C1 16 16 F LUXURY SMALL C1 17 17 F LUXURY MEDIUM C1 18 18 F LUXURY MEDIUM C1 19 19 F LUXURY MEDIUM C1 20 20 F LUXURY LARGE C1 pertama mencari entropy(s) dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"Cara Membuat Decision Tree"},{"location":"Statistik Dekriptif/","text":"Statistik Deskriptif \u00b6 Pengertian \u00b6 Statistik deskriptif adalah koefisien deskriptif singkat yang meringkas set data yang diberikan, yang dapat berupa representasi keseluruhan atau sampel populasi. Statistik deskriptif dipecah menjadi ukuran tendensi sentral dan ukuran variabilitas (sebaran). Ukuran tendensi sentral meliputi mean, median, dan mode, sedangkan ukuran variabilitas meliputi deviasi standar, varians, variabel minimum dan maksimum, dan kurtosis dan skewness. Tipe Statistik Deskriptif \u00b6 Mean (rata-rata) \u00b6 Mean merupakan rata-rata dari semua angka. Mean didapat dari penjumlahan keseluruhan angka dibagi dengan banyaknya angka itu sendiri. Jika kita memiliki N data, kita dapat menghitung mean itu sendiri dengan menggunakan rumus berikut : $$ \u00afx=n\u2211i=1xiN=x1+x2+x3+...+xnN $$ Dimana: x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyaknya data Median \u00b6 Median merupakan pusat data atau lebih sering dikatakan nilai tengah dari sebuah urutan data. Median disimbolkan dengan Me . nilai dari median akan sama dengan nilai Quartile 2 ( Q2 ). Untuk mencari median kita dapat menggunakan rumus sebagai berikut : $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Median dari kelompok data n = banyak data Modus \u00b6 Modus adalah angka yang paling sering ditemukan dalam suatu himpunan angka. Modus didapat dengan mengumpulkan dan mengatur data untuk menghitung setiap frekuensi dari setiap hasil. Hasil dengan jumlah tertinggi merupakan modus. Untuk mencari modus dari sebuah himpunan angka dapat menggunakan rumus berikut : $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak (selalu positif) Varians \u00b6 Varian adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Untuk menghitung varian terdapat formula yang dapat digunakan yaitu : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Dimana : xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data Standar Deviasi \u00b6 Standar deviasi merupakan ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Untuk mencari standar deviasi kita dapat menggunakan formula berikut : $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Skewness \u00b6 Skewness ( kemiringan ) adalah ketidaksimetrisan pada suatu distribusi statistik dimana kurva tampak condong ke kiri atau ke kanan. Skewness digunakan untuk menentukan sejauh mana perbedaan suatu distribusi dengan distribusi normal. Skewness bisa dihitung menggunakan rumus sebagai berikut : $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Dimana : xi = titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi Quartile \u00b6 Quartile adalah irisan nilai dari hasil pembagian data menjadi empat bagian yang sama besar. Yaitu Q1, Q2, Q3, dan Q4. Dalam mencari quatile kita dapat menggunakan rumus berikut ini : $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Dimana : Q = Nilai dari quartile n = banyak dari himpunan data Penerapan Statistik Deskriptif Menggunakan Python \u00b6 Alat dan Bahan \u00b6 Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. Dalam kasus ini, library python yang digunakan adalah pandas dan scipy. import pandas as pd from scipy import stats df = pd . read_csv ( 'sampel.csv' , sep = ';' ) data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standar Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'Stats' ] + [ x for x in df . columns ]) tes Berikut hasil gabungan dari code yang telah di buat untuk menampilkan program tabel dibawah ini : Stats 2016 2017 2018 2019 0 Min 20.000 30.000 10.000 15.000 1 Max 98.000 94.000 100.000 96.000 2 Mean 57.826 61.878 55.622 53.432 3 Standar Deviasi 22.280 18.700 25.910 22.920 4 Variasi 496.580 349.650 671.110 525.110 5 Skewnes 0.090 0.040 -0.070 0.110 6 Quantile 1 39.000 46.000 33.000 34.000 7 Quantile 2 58.000 62.000 58.500 54.000 8 Quantile 3 77.000 77.250 76.250 72.000 9 Median 58.000 62.000 58.500 54.000 10 Modus 65.000 33.000 77.000 31.000 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Statistik Deskriptif"},{"location":"Statistik Dekriptif/#statistik-deskriptif","text":"","title":"Statistik Deskriptif"},{"location":"Statistik Dekriptif/#pengertian","text":"Statistik deskriptif adalah koefisien deskriptif singkat yang meringkas set data yang diberikan, yang dapat berupa representasi keseluruhan atau sampel populasi. Statistik deskriptif dipecah menjadi ukuran tendensi sentral dan ukuran variabilitas (sebaran). Ukuran tendensi sentral meliputi mean, median, dan mode, sedangkan ukuran variabilitas meliputi deviasi standar, varians, variabel minimum dan maksimum, dan kurtosis dan skewness.","title":"Pengertian"},{"location":"Statistik Dekriptif/#tipe-statistik-deskriptif","text":"","title":"Tipe Statistik Deskriptif"},{"location":"Statistik Dekriptif/#mean-rata-rata","text":"Mean merupakan rata-rata dari semua angka. Mean didapat dari penjumlahan keseluruhan angka dibagi dengan banyaknya angka itu sendiri. Jika kita memiliki N data, kita dapat menghitung mean itu sendiri dengan menggunakan rumus berikut : $$ \u00afx=n\u2211i=1xiN=x1+x2+x3+...+xnN $$ Dimana: x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyaknya data","title":"Mean (rata-rata)"},{"location":"Statistik Dekriptif/#median","text":"Median merupakan pusat data atau lebih sering dikatakan nilai tengah dari sebuah urutan data. Median disimbolkan dengan Me . nilai dari median akan sama dengan nilai Quartile 2 ( Q2 ). Untuk mencari median kita dapat menggunakan rumus sebagai berikut : $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Median dari kelompok data n = banyak data","title":"Median"},{"location":"Statistik Dekriptif/#modus","text":"Modus adalah angka yang paling sering ditemukan dalam suatu himpunan angka. Modus didapat dengan mengumpulkan dan mengatur data untuk menghitung setiap frekuensi dari setiap hasil. Hasil dengan jumlah tertinggi merupakan modus. Untuk mencari modus dari sebuah himpunan angka dapat menggunakan rumus berikut : $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak (selalu positif)","title":"Modus"},{"location":"Statistik Dekriptif/#varians","text":"Varian adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Untuk menghitung varian terdapat formula yang dapat digunakan yaitu : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Dimana : xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data","title":"Varians"},{"location":"Statistik Dekriptif/#standar-deviasi","text":"Standar deviasi merupakan ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Untuk mencari standar deviasi kita dapat menggunakan formula berikut : $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$","title":"Standar Deviasi"},{"location":"Statistik Dekriptif/#skewness","text":"Skewness ( kemiringan ) adalah ketidaksimetrisan pada suatu distribusi statistik dimana kurva tampak condong ke kiri atau ke kanan. Skewness digunakan untuk menentukan sejauh mana perbedaan suatu distribusi dengan distribusi normal. Skewness bisa dihitung menggunakan rumus sebagai berikut : $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Dimana : xi = titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi","title":"Skewness"},{"location":"Statistik Dekriptif/#quartile","text":"Quartile adalah irisan nilai dari hasil pembagian data menjadi empat bagian yang sama besar. Yaitu Q1, Q2, Q3, dan Q4. Dalam mencari quatile kita dapat menggunakan rumus berikut ini : $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Dimana : Q = Nilai dari quartile n = banyak dari himpunan data","title":"Quartile"},{"location":"Statistik Dekriptif/#penerapan-statistik-deskriptif-menggunakan-python","text":"","title":"Penerapan Statistik Deskriptif Menggunakan Python"},{"location":"Statistik Dekriptif/#alat-dan-bahan","text":"Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. Dalam kasus ini, library python yang digunakan adalah pandas dan scipy. import pandas as pd from scipy import stats df = pd . read_csv ( 'sampel.csv' , sep = ';' ) data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standar Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'Stats' ] + [ x for x in df . columns ]) tes Berikut hasil gabungan dari code yang telah di buat untuk menampilkan program tabel dibawah ini : Stats 2016 2017 2018 2019 0 Min 20.000 30.000 10.000 15.000 1 Max 98.000 94.000 100.000 96.000 2 Mean 57.826 61.878 55.622 53.432 3 Standar Deviasi 22.280 18.700 25.910 22.920 4 Variasi 496.580 349.650 671.110 525.110 5 Skewnes 0.090 0.040 -0.070 0.110 6 Quantile 1 39.000 46.000 33.000 34.000 7 Quantile 2 58.000 62.000 58.500 54.000 8 Quantile 3 77.000 77.250 76.250 72.000 9 Median 58.000 62.000 58.500 54.000 10 Modus 65.000 33.000 77.000 31.000 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Alat dan Bahan"},{"location":"clustering/","text":"Clustering Categorical Data \u00b6 Clustering adalah sebuah proses untuk mengelompokan data ke dalam beberapa cluster atau kelompok sehingga data dalam satu cluster memiliki tingkat kemiripan yang maksimum dan data antar cluster memiliki kemiripan yang minimum. Metode K-Means Clustering \u00b6 K-Means adalah salah satu algoritma clustering / pengelompokan data yang bersifat Unsupervised Learning, yang berarti masukan dari algoritma ini menerima data tanpa label kelas. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. Karakteristik dari algoritma ini adalah : Memiliki n buah data Input berupa jumlah data dan jumlah cluster (kelompok) Pada setiap cluster / kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Algoritma K-Means \u00b6 Secara sederhana algoritma K-Means dimulai dari tahap berikut : Pilih K buah titik centroid. Menghitung jarak data dengan centroid. Update nilai titik centroid. Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah. Rumus K-Means \u00b6 Metode K-Modes \u00b6 K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster. Rumus K-Modes \u00b6 Metode K-Prototype \u00b6 Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu. Rumus K-Prototype \u00b6 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering"},{"location":"clustering/#clustering-categorical-data","text":"Clustering adalah sebuah proses untuk mengelompokan data ke dalam beberapa cluster atau kelompok sehingga data dalam satu cluster memiliki tingkat kemiripan yang maksimum dan data antar cluster memiliki kemiripan yang minimum.","title":"Clustering Categorical Data"},{"location":"clustering/#metode-k-means-clustering","text":"K-Means adalah salah satu algoritma clustering / pengelompokan data yang bersifat Unsupervised Learning, yang berarti masukan dari algoritma ini menerima data tanpa label kelas. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. Karakteristik dari algoritma ini adalah : Memiliki n buah data Input berupa jumlah data dan jumlah cluster (kelompok) Pada setiap cluster / kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut.","title":"Metode K-Means Clustering"},{"location":"clustering/#algoritma-k-means","text":"Secara sederhana algoritma K-Means dimulai dari tahap berikut : Pilih K buah titik centroid. Menghitung jarak data dengan centroid. Update nilai titik centroid. Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah.","title":"Algoritma K-Means"},{"location":"clustering/#rumus-k-means","text":"","title":"Rumus K-Means"},{"location":"clustering/#metode-k-modes","text":"K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster.","title":"Metode K-Modes"},{"location":"clustering/#rumus-k-modes","text":"","title":"Rumus K-Modes"},{"location":"clustering/#metode-k-prototype","text":"Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu.","title":"Metode K-Prototype"},{"location":"clustering/#rumus-k-prototype","text":"MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Rumus K-Prototype"},{"location":"fuzzy/","text":"Fuzzy C-Means \u00b6 Pengertian \u00b6 Fuzzy clustering adalah proses menentukan derajat keanggotaan, dan kemudian menggunakannya dengan memasukkannya kedalam elemen data kedalam satu kelompok cluster atau lebih. Hal ini akan memberikan informasi kesamaan dari setiap objek. Satu dari sekian banyaknya algoritma fuzzy clustering yang digunakan adalah algoritma fuzzy clustering c means. Vektor dari fuzzy clustering, V={v1, v2, v3,\u2026, vc}, merupakan sebuah fungsi objektif yang di defenisikan dengan derajat keanggotaan dari data Xj dan pusat cluster Vj. Algoritma fuzzy clustering c means membagi data yang tersedia dari setiap elemen data berhingga lalu memasukkannya kedalam bagian dari koleksi cluster yang dipengaruhi oleh beberapa kriteria yang diberikan. Berikan satu kumpulan data berhingga. X= {x1,\u2026, xn } dan pusat data. Dimana \u03bc ij adalah derajat keanggotaan dari Xj dan pusat cluster adalah sebuah bagian dari keanggotaan matriks [\u03bc ij]. d2 adalah akar dari Euclidean distance dan m adalah parameter fuzzy yang rata-rata derajat kekaburan dari setiap data derajat keanggotaan tidak lebih besar dari 1,0 Output dari Fuzzy C-Means merupakan deretan pusat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Algoritma Fuzzy C-Means \u00b6 Secara sederhana algoritma K-Means dimulai dari tahap berikut : Input data yang akan dicluster X, berupa matriks berukuran n x m (n=jumlah sample data, m=atribut setiap data). Xij=data sample ke-i (i=1,2,\u2026,n), atribut ke-j (j=1,2,\u2026,m). Tentukan : Jumlah cluster = c Pangkat = w Maksimum iterasi = MaxIter Error terkecil yang diharapkan = \u03be Fungsi obyektif awal = Po = 0 Iterasi awal = t Bangkitkan nilai acak \u03bcik, i=1,2,\u2026,n; k=1,2,\u2026,c sebagai elemen-elemen matriks partisi awal \u03bcik. \u03bcik adalah derajat keanggotaan yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster.Posisi dan nilai matriks dibangun secara random. Dimana nilai keangotaan terletak pada interval 0 sampai dengan 1. Pada posisi awal matriks partisi U masih belum akurat begitu juga pusat clusternya. Sehingga kecendrungan data untuk masuk suatu cluster juga belum akurat. Input data yang akan dicluster X, berupa matriks berukuran n x m (n=jumlah sample data, m=atribut setiap data). Xij=data sample ke-i (i=1,2,\u2026,n), atribut ke-j (j=1,2,\u2026,m). Fungsi objektif digunakan sebagai syarat perulangan untuk mendapatkan pusat cluster yang tepat. Sehingga diperoleh kecendrungan data untuk masuk ke *cluster* mana pada *step* akhir. Hitung fungsi obyektif pada iterasi ke-t, Pt Perhitungan fungsi objektif Pt dimana nilai variabel fuzzy Xij di kurang dengan dengan pusat cluster Vkj kemudian hasil pengurangannya di kuadradkan lalu masing-masing hasil kuadrad di jumlahkan untuk dikali dengan kuadrad dari derajat keanggotaan \u03bcik untuk tiap cluster. Setelah itu jumlahkan semua nilai di semua cluster untuk mendapatkan fungsi objektif Pt. Hitung perubahan matriks partisi: Cek kondisi berhenti:a) jika:( |Pt \u2013 Pt-1 | < \u03be) atau (t>maxIter) maka berhenti.b) jika tidak, t=t+1, ulangi langkah ke-4. Contoh Code Fuzzy C-Means \u00b6 from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"Fuzzy C-Means"},{"location":"fuzzy/#fuzzy-c-means","text":"","title":"Fuzzy C-Means"},{"location":"fuzzy/#pengertian","text":"Fuzzy clustering adalah proses menentukan derajat keanggotaan, dan kemudian menggunakannya dengan memasukkannya kedalam elemen data kedalam satu kelompok cluster atau lebih. Hal ini akan memberikan informasi kesamaan dari setiap objek. Satu dari sekian banyaknya algoritma fuzzy clustering yang digunakan adalah algoritma fuzzy clustering c means. Vektor dari fuzzy clustering, V={v1, v2, v3,\u2026, vc}, merupakan sebuah fungsi objektif yang di defenisikan dengan derajat keanggotaan dari data Xj dan pusat cluster Vj. Algoritma fuzzy clustering c means membagi data yang tersedia dari setiap elemen data berhingga lalu memasukkannya kedalam bagian dari koleksi cluster yang dipengaruhi oleh beberapa kriteria yang diberikan. Berikan satu kumpulan data berhingga. X= {x1,\u2026, xn } dan pusat data. Dimana \u03bc ij adalah derajat keanggotaan dari Xj dan pusat cluster adalah sebuah bagian dari keanggotaan matriks [\u03bc ij]. d2 adalah akar dari Euclidean distance dan m adalah parameter fuzzy yang rata-rata derajat kekaburan dari setiap data derajat keanggotaan tidak lebih besar dari 1,0 Output dari Fuzzy C-Means merupakan deretan pusat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system.","title":"Pengertian"},{"location":"fuzzy/#algoritma-fuzzy-c-means","text":"Secara sederhana algoritma K-Means dimulai dari tahap berikut : Input data yang akan dicluster X, berupa matriks berukuran n x m (n=jumlah sample data, m=atribut setiap data). Xij=data sample ke-i (i=1,2,\u2026,n), atribut ke-j (j=1,2,\u2026,m). Tentukan : Jumlah cluster = c Pangkat = w Maksimum iterasi = MaxIter Error terkecil yang diharapkan = \u03be Fungsi obyektif awal = Po = 0 Iterasi awal = t Bangkitkan nilai acak \u03bcik, i=1,2,\u2026,n; k=1,2,\u2026,c sebagai elemen-elemen matriks partisi awal \u03bcik. \u03bcik adalah derajat keanggotaan yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster.Posisi dan nilai matriks dibangun secara random. Dimana nilai keangotaan terletak pada interval 0 sampai dengan 1. Pada posisi awal matriks partisi U masih belum akurat begitu juga pusat clusternya. Sehingga kecendrungan data untuk masuk suatu cluster juga belum akurat. Input data yang akan dicluster X, berupa matriks berukuran n x m (n=jumlah sample data, m=atribut setiap data). Xij=data sample ke-i (i=1,2,\u2026,n), atribut ke-j (j=1,2,\u2026,m). Fungsi objektif digunakan sebagai syarat perulangan untuk mendapatkan pusat cluster yang tepat. Sehingga diperoleh kecendrungan data untuk masuk ke *cluster* mana pada *step* akhir. Hitung fungsi obyektif pada iterasi ke-t, Pt Perhitungan fungsi objektif Pt dimana nilai variabel fuzzy Xij di kurang dengan dengan pusat cluster Vkj kemudian hasil pengurangannya di kuadradkan lalu masing-masing hasil kuadrad di jumlahkan untuk dikali dengan kuadrad dari derajat keanggotaan \u03bcik untuk tiap cluster. Setelah itu jumlahkan semua nilai di semua cluster untuk mendapatkan fungsi objektif Pt. Hitung perubahan matriks partisi: Cek kondisi berhenti:a) jika:( |Pt \u2013 Pt-1 | < \u03be) atau (t>maxIter) maka berhenti.b) jika tidak, t=t+1, ulangi langkah ke-4.","title":"Algoritma Fuzzy C-Means"},{"location":"fuzzy/#contoh-code-fuzzy-c-means","text":"from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"Contoh Code Fuzzy C-Means"}]}