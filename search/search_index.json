{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang di Penambangan Data \u00b6 Nama : Anggrahito Cahyo Dwi Prasetyaningtyas NIM : 180411100019 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab, S.Si., M.Kom Jurusan : Teknik Informatika","title":"Index"},{"location":"#selamat-datang-di-penambangan-data","text":"Nama : Anggrahito Cahyo Dwi Prasetyaningtyas NIM : 180411100019 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab, S.Si., M.Kom Jurusan : Teknik Informatika","title":"Selamat Datang di Penambangan Data"},{"location":"Eliminasi Gauss/","text":"Metode Eliminasi Gauss \u00b6 Eliminasi Gauss adalah suatu cara mengoperasikan nilai-nilai di dalam matriks sehingga menjadi matriks yang lebih sederhana. Caranya adalah dengan melakukan operasi baris sehingga matriks tersebut menjadi matriks yang eselon-baris. Ini dapat digunakan sebagai salah satu metode penyelesaian persamaan linear dengan menggunakan matriks. Caranya dengan mengubah persamaan linear tersebut ke dalam matriks teraugmentasi dan mengoperasikannya. Setelah menjadi matriks Eselon-baris, lakukan substitusi balik untuk mendapatkan nilai dari variabel-variabel tersebut. Metode ini berangkat dari kenyataan bahwa bila matriks A berbentuk segitiga atas (menggunakan Operasi Baris Elementer) seperti system persamaan berikut ini: Maka solusinya dapat dihitung dengan teknik penyulingan mundur ( backward substitution ): kondisi sangat penting. Sebab bila persamaan diatas menjerjakan pembagian dengan nol. Apabila kondisi tersebut tidak dipenuhi, maka SPL tidak mempunyai jawaban Eliminasi Gauss Jordan \u00b6 Dalam aljabar linear, eliminasi Gauss-Jordan adalah versi dari eliminasi Gauss. Pada metode eliminasi Gauss-Jordan kita membuat nol elemen-elemen di bawah maupun di atas diagonal utama suatu matriks. Hasilnya adalah matriks tereduksi yang berupa matriks diagonal satuan (semua elemen pada diagonal utama bernilai 1, elemen-elemen lainnya nol). Dalam bentuk matriks, eliminasi Gauss-Jordan ditulis sebagai berikut: Listing Program : import numpy as np #Definisi Matrix A = [] B = [] n = int(input(\"Masukkan ukuran Matrix: \")) for i in range(n): baris=[] for i in range(n): a=int(input(\"Masukkan Nilai: \")) baris.append(a) A.append(baris) for i in range(n): h = int(input(\"Masukkan Hasil: \")) B.append(h) Matrix=np.array(A,float) Hasil=np.array(B,float) n=len(Matrix) #Eliminasi Gauss for k in range(0,n-1): for i in range(k+1,n): if Matrix[i,k]!=0 : lam=Matrix[i,k]/Matrix[k,k] Matrix[i,k:n]=Matrix[i,k:n]-(Matrix[k,k:n]*lam) Hasil[i]=Hasil[i]-(Hasil[k]*lam) print(\"Matrix A : \",'\\n',Matrix) #Subtitution x=np.zeros(n,float) for m in range(n-1,-1,-1): x[m]=(Hasil[m]-np.dot(Matrix[m, m+1:n], x[m+1:n]))/Matrix[m,m] print('Nilai X ',m+1, '=',x[m]) Masukkan ukuran Matrix : 3 Masukkan Nilai : 2 Masukkan Nilai : - 2 Masukkan Nilai : 5 Masukkan Nilai : 1 Masukkan Nilai : 5 Masukkan Nilai : 2 Masukkan Nilai : 4 Masukkan Nilai : 5 Masukkan Nilai : 2 Masukkan Hasil : 12 Masukkan Hasil : 3 Masukkan Hasil : - 4 Matrix A : [[ 2. - 2. 5. ] [ 0. 6. - 0.5 ] [ 0. 0. - 7.25 ]] Nilai X 3 = 3.2413793103448274 Nilai X 2 = - 0.2298850574712644 Nilai X 1 = - 2.333333333333332 Jadi panjang Matrix yang dibuat dalam Program Diatas adalah 3 variabel. |2 -2 5| |12| |1 5 2|=| 3 | |4 5 2| |-4| Pivot yang dibentuk adalah a1.1,a2.2,dan a3.3 sehingga semua angka yang ada dibawah pivot akan dikonversikan menjadi nol sesuai hasil program dan hasil dari persamaan diatas menghasilkan x1=-2.333333333, x2=-0.22988505 dan x3=3.2413793 Eliminasi Gauss Jacobi \u00b6 Metode Jacobi, adalah metode tak langsung atau metode iteratif yang melakukan perbaharuan nilai x yang diperoleh tiap iterasi (mirip metode substitusi berurutan). Metode ini hampir sama dengan metode Gauss Seidel, namun tidak melibatkan perhitungan implisit. Metode ini merupakan suatu teknik penyelesaian SPL berukuran n x n, AX = b, secara iteratif. Proses penyelesaian dimulai dengan suatu hampiran awal terhadap penyelesaian, X0, kemudian membentuk suatu serangkaian vector X1, X2, \u2026 yang konvergen ke X. Metode ini ditemukan oleh Matematikawan yang berasal dari Jerman,Carl,Gustav,Jacobi. Penemuan ini diperkirakan pada tahun 1800-an Listing Program : from pprint import pprint from numpy import array , zeros , diag , diagflat , dot import numpy as np def jacobi ( A , b , N = 25 , x = None ): #Membuat iniial guess if x is None : x = zeros ( len ( A [ 0 ])) #Membuat vektor dari elemen matrix A D = diag ( A ) R = A - diagflat ( D ) #Iterasi for i in range ( N ): x = ( b - dot ( R , x )) / D return x Mat1 = [] Mat2 = [] n = int ( input ( \"Masukkan ukuran Matrix: \" )) for i in range ( n ): baris = [] for i in range ( n ): a = int ( input ( \"Masukkan Nilai: \" )) baris . append ( a ) Mat1 . append ( baris ) for i in range ( n ): h = int ( input ( \"Masukkan Hasil: \" )) Mat2 . append ( h ) A = array ( Mat1 , float ) b = array ( Mat2 , float ) x = len ( Mat1 ) guess = np . zeros ( x , float ) sol = jacobi ( A , b , N = 25 , x = guess ) print ( \"A:\" ) pprint ( A ) print ( \"b:\" ) pprint ( b ) print ( \"x:\" ) pprint ( sol ) Masukkan ukuran Matrix : 3 Masukkan Nilai : 3 Masukkan Nilai : 1 Masukkan Nilai : - 1 Masukkan Nilai : 4 Masukkan Nilai : 7 Masukkan Nilai : - 3 Masukkan Nilai : 2 Masukkan Nilai : - 2 Masukkan Nilai : 5 Masukkan Hasil : 5 Masukkan Hasil : 20 Masukkan Hasil : 10 A : array ([[ 3. , 1. , - 1. ], [ 4. , 7. , - 3. ], [ 2. , - 2. , 5. ]]) b : array ([ 5. , 20. , 10. ]) x : array ([ 1.50602413 , 3.13253016 , 2.6506024 ]) Eliminasi Gauss Seidel \u00b6 Metode iterasi Gauss-Seidel adalah metode yang menggunakan proses iterasi hingga diperoleh nilai-nilai yang berubah-ubah dan akhirnya relatif konstan. Metode iterasi Gauss-Seidel dikembangkan dari gagasan metode iterasi pada solusi persamaan tak linier. Gauss ini mempunyai kelebihan dan kekurangan. kelebihannya yaitu Metode eliminasi gauss-seidel digunakan untuk menyelesaikan SPL yang berukuran kecil karena metode ini lebih efisien. Dengan metode iterasi Gauss-Seidel toleransi pembulatan dapat diperkecil karena iterasi dapat diteruskan sampai seteliti mungkin sesuai dengan batas toleransi yang diinginkan. kekurangannya yaitu Kelemahan dari metode ini adalah masalah pivot (titik tengah) yang harus benar\u2013benar diperhatikan, karena penyusunan yang salah akan menyebabkan iterasi menjadi divergen dan tidak diperoleh hasil yang benar Listing Program : def seidel ( a , x , b ): #Mencari Panjang Matrix n = len ( a ) for j in range ( 0 , n ): d = b [ j ] #Menghitung xi, yi, zi for i in range ( 0 , n ): if ( j != i ): d -= a [ j ][ i ] * x [ i ] x [ j ] = d / a [ j ][ j ] #Solusi return x m = int ( input ( \"Masukkan Panjang Matrix: \" )) a = [] b = [] for k in range ( m ): mat1 = [] for i in range ( m ): l = float ( input ( \"Masukkan a\" + str ( k + 1 ) + \",\" + str ( i + 1 ) + \": \" )) mat1 . append ( l ) h = float ( input ( \"Masukkan Hasil: \" )) b . append ( h ) a . append ( mat1 ) n = 3 x = [ 0 , 0 , 0 ] print ( x ) for i in range ( 0 , 100 ): x = seidel ( a , x , b ) print ( x ) Masukkan Panjang Matrix : 3 Masukkan a1 , 1 : 4 Masukkan a1 , 2 : - 1 Masukkan a1 , 3 : 1 Masukkan Hasil : 7 Masukkan a2 , 1 : 4 Masukkan a2 , 2 : - 8 Masukkan a2 , 3 : 1 Masukkan Hasil : - 21 Masukkan a3 , 1 : - 2 Masukkan a3 , 2 : 1 Masukkan a3 , 3 : 5 Masukkan Hasil : 15 [ 0 , 0 , 0 ] [ 1.75 , 3.5 , 3.0 ] [ 1.875 , 3.9375 , 2.9625 ] [ 1.99375 , 3.9921875 , 2.9990625 ] [ 1.99828125 , 3.9990234375 , 2.9995078125 ] [ 1.99987890625 , 3.9998779296875 , 2.9999759765625003 ] [ 1.99997548828125 , 3.9999847412109375 , 2.999993247070312 ] [ 1.9999978735351562 , 3.9999980926513667 , 2.999999530883789 ] [ 1.9999996404418945 , 3.9999997615814205 , 2.9999999038604734 ] [ 1.9999999644302369 , 3.9999999701976776 , 2.9999999917325595 ] [ 1.9999999946162794 , 3.9999999962747097 , 2.99999999859157 ] [ 1.9999999994207849 , 3.9999999995343387 , 2.9999999998614464 ] [ 1.9999999999182232 , 3.999999999941793 , 2.999999999978931 ] [ 1.9999999999907154 , 3.999999999992724 , 2.9999999999977414 ] [ 1.9999999999987457 , 3.9999999999990905 , 2.9999999999996803 ] [ 1.9999999999998526 , 3.9999999999998863 , 2.9999999999999636 ] [ 1.9999999999999807 , 3.999999999999986 , 2.999999999999995 ] [ 1.9999999999999978 , 3.9999999999999987 , 2.9999999999999996 ] [ 1.9999999999999996 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ]","title":"Eliminasi Gauss"},{"location":"Eliminasi Gauss/#metode-eliminasi-gauss","text":"Eliminasi Gauss adalah suatu cara mengoperasikan nilai-nilai di dalam matriks sehingga menjadi matriks yang lebih sederhana. Caranya adalah dengan melakukan operasi baris sehingga matriks tersebut menjadi matriks yang eselon-baris. Ini dapat digunakan sebagai salah satu metode penyelesaian persamaan linear dengan menggunakan matriks. Caranya dengan mengubah persamaan linear tersebut ke dalam matriks teraugmentasi dan mengoperasikannya. Setelah menjadi matriks Eselon-baris, lakukan substitusi balik untuk mendapatkan nilai dari variabel-variabel tersebut. Metode ini berangkat dari kenyataan bahwa bila matriks A berbentuk segitiga atas (menggunakan Operasi Baris Elementer) seperti system persamaan berikut ini: Maka solusinya dapat dihitung dengan teknik penyulingan mundur ( backward substitution ): kondisi sangat penting. Sebab bila persamaan diatas menjerjakan pembagian dengan nol. Apabila kondisi tersebut tidak dipenuhi, maka SPL tidak mempunyai jawaban","title":"Metode Eliminasi Gauss"},{"location":"Eliminasi Gauss/#eliminasi-gauss-jordan","text":"Dalam aljabar linear, eliminasi Gauss-Jordan adalah versi dari eliminasi Gauss. Pada metode eliminasi Gauss-Jordan kita membuat nol elemen-elemen di bawah maupun di atas diagonal utama suatu matriks. Hasilnya adalah matriks tereduksi yang berupa matriks diagonal satuan (semua elemen pada diagonal utama bernilai 1, elemen-elemen lainnya nol). Dalam bentuk matriks, eliminasi Gauss-Jordan ditulis sebagai berikut: Listing Program : import numpy as np #Definisi Matrix A = [] B = [] n = int(input(\"Masukkan ukuran Matrix: \")) for i in range(n): baris=[] for i in range(n): a=int(input(\"Masukkan Nilai: \")) baris.append(a) A.append(baris) for i in range(n): h = int(input(\"Masukkan Hasil: \")) B.append(h) Matrix=np.array(A,float) Hasil=np.array(B,float) n=len(Matrix) #Eliminasi Gauss for k in range(0,n-1): for i in range(k+1,n): if Matrix[i,k]!=0 : lam=Matrix[i,k]/Matrix[k,k] Matrix[i,k:n]=Matrix[i,k:n]-(Matrix[k,k:n]*lam) Hasil[i]=Hasil[i]-(Hasil[k]*lam) print(\"Matrix A : \",'\\n',Matrix) #Subtitution x=np.zeros(n,float) for m in range(n-1,-1,-1): x[m]=(Hasil[m]-np.dot(Matrix[m, m+1:n], x[m+1:n]))/Matrix[m,m] print('Nilai X ',m+1, '=',x[m]) Masukkan ukuran Matrix : 3 Masukkan Nilai : 2 Masukkan Nilai : - 2 Masukkan Nilai : 5 Masukkan Nilai : 1 Masukkan Nilai : 5 Masukkan Nilai : 2 Masukkan Nilai : 4 Masukkan Nilai : 5 Masukkan Nilai : 2 Masukkan Hasil : 12 Masukkan Hasil : 3 Masukkan Hasil : - 4 Matrix A : [[ 2. - 2. 5. ] [ 0. 6. - 0.5 ] [ 0. 0. - 7.25 ]] Nilai X 3 = 3.2413793103448274 Nilai X 2 = - 0.2298850574712644 Nilai X 1 = - 2.333333333333332 Jadi panjang Matrix yang dibuat dalam Program Diatas adalah 3 variabel. |2 -2 5| |12| |1 5 2|=| 3 | |4 5 2| |-4| Pivot yang dibentuk adalah a1.1,a2.2,dan a3.3 sehingga semua angka yang ada dibawah pivot akan dikonversikan menjadi nol sesuai hasil program dan hasil dari persamaan diatas menghasilkan x1=-2.333333333, x2=-0.22988505 dan x3=3.2413793","title":"Eliminasi Gauss Jordan"},{"location":"Eliminasi Gauss/#eliminasi-gauss-jacobi","text":"Metode Jacobi, adalah metode tak langsung atau metode iteratif yang melakukan perbaharuan nilai x yang diperoleh tiap iterasi (mirip metode substitusi berurutan). Metode ini hampir sama dengan metode Gauss Seidel, namun tidak melibatkan perhitungan implisit. Metode ini merupakan suatu teknik penyelesaian SPL berukuran n x n, AX = b, secara iteratif. Proses penyelesaian dimulai dengan suatu hampiran awal terhadap penyelesaian, X0, kemudian membentuk suatu serangkaian vector X1, X2, \u2026 yang konvergen ke X. Metode ini ditemukan oleh Matematikawan yang berasal dari Jerman,Carl,Gustav,Jacobi. Penemuan ini diperkirakan pada tahun 1800-an Listing Program : from pprint import pprint from numpy import array , zeros , diag , diagflat , dot import numpy as np def jacobi ( A , b , N = 25 , x = None ): #Membuat iniial guess if x is None : x = zeros ( len ( A [ 0 ])) #Membuat vektor dari elemen matrix A D = diag ( A ) R = A - diagflat ( D ) #Iterasi for i in range ( N ): x = ( b - dot ( R , x )) / D return x Mat1 = [] Mat2 = [] n = int ( input ( \"Masukkan ukuran Matrix: \" )) for i in range ( n ): baris = [] for i in range ( n ): a = int ( input ( \"Masukkan Nilai: \" )) baris . append ( a ) Mat1 . append ( baris ) for i in range ( n ): h = int ( input ( \"Masukkan Hasil: \" )) Mat2 . append ( h ) A = array ( Mat1 , float ) b = array ( Mat2 , float ) x = len ( Mat1 ) guess = np . zeros ( x , float ) sol = jacobi ( A , b , N = 25 , x = guess ) print ( \"A:\" ) pprint ( A ) print ( \"b:\" ) pprint ( b ) print ( \"x:\" ) pprint ( sol ) Masukkan ukuran Matrix : 3 Masukkan Nilai : 3 Masukkan Nilai : 1 Masukkan Nilai : - 1 Masukkan Nilai : 4 Masukkan Nilai : 7 Masukkan Nilai : - 3 Masukkan Nilai : 2 Masukkan Nilai : - 2 Masukkan Nilai : 5 Masukkan Hasil : 5 Masukkan Hasil : 20 Masukkan Hasil : 10 A : array ([[ 3. , 1. , - 1. ], [ 4. , 7. , - 3. ], [ 2. , - 2. , 5. ]]) b : array ([ 5. , 20. , 10. ]) x : array ([ 1.50602413 , 3.13253016 , 2.6506024 ])","title":"Eliminasi Gauss Jacobi"},{"location":"Eliminasi Gauss/#eliminasi-gauss-seidel","text":"Metode iterasi Gauss-Seidel adalah metode yang menggunakan proses iterasi hingga diperoleh nilai-nilai yang berubah-ubah dan akhirnya relatif konstan. Metode iterasi Gauss-Seidel dikembangkan dari gagasan metode iterasi pada solusi persamaan tak linier. Gauss ini mempunyai kelebihan dan kekurangan. kelebihannya yaitu Metode eliminasi gauss-seidel digunakan untuk menyelesaikan SPL yang berukuran kecil karena metode ini lebih efisien. Dengan metode iterasi Gauss-Seidel toleransi pembulatan dapat diperkecil karena iterasi dapat diteruskan sampai seteliti mungkin sesuai dengan batas toleransi yang diinginkan. kekurangannya yaitu Kelemahan dari metode ini adalah masalah pivot (titik tengah) yang harus benar\u2013benar diperhatikan, karena penyusunan yang salah akan menyebabkan iterasi menjadi divergen dan tidak diperoleh hasil yang benar Listing Program : def seidel ( a , x , b ): #Mencari Panjang Matrix n = len ( a ) for j in range ( 0 , n ): d = b [ j ] #Menghitung xi, yi, zi for i in range ( 0 , n ): if ( j != i ): d -= a [ j ][ i ] * x [ i ] x [ j ] = d / a [ j ][ j ] #Solusi return x m = int ( input ( \"Masukkan Panjang Matrix: \" )) a = [] b = [] for k in range ( m ): mat1 = [] for i in range ( m ): l = float ( input ( \"Masukkan a\" + str ( k + 1 ) + \",\" + str ( i + 1 ) + \": \" )) mat1 . append ( l ) h = float ( input ( \"Masukkan Hasil: \" )) b . append ( h ) a . append ( mat1 ) n = 3 x = [ 0 , 0 , 0 ] print ( x ) for i in range ( 0 , 100 ): x = seidel ( a , x , b ) print ( x ) Masukkan Panjang Matrix : 3 Masukkan a1 , 1 : 4 Masukkan a1 , 2 : - 1 Masukkan a1 , 3 : 1 Masukkan Hasil : 7 Masukkan a2 , 1 : 4 Masukkan a2 , 2 : - 8 Masukkan a2 , 3 : 1 Masukkan Hasil : - 21 Masukkan a3 , 1 : - 2 Masukkan a3 , 2 : 1 Masukkan a3 , 3 : 5 Masukkan Hasil : 15 [ 0 , 0 , 0 ] [ 1.75 , 3.5 , 3.0 ] [ 1.875 , 3.9375 , 2.9625 ] [ 1.99375 , 3.9921875 , 2.9990625 ] [ 1.99828125 , 3.9990234375 , 2.9995078125 ] [ 1.99987890625 , 3.9998779296875 , 2.9999759765625003 ] [ 1.99997548828125 , 3.9999847412109375 , 2.999993247070312 ] [ 1.9999978735351562 , 3.9999980926513667 , 2.999999530883789 ] [ 1.9999996404418945 , 3.9999997615814205 , 2.9999999038604734 ] [ 1.9999999644302369 , 3.9999999701976776 , 2.9999999917325595 ] [ 1.9999999946162794 , 3.9999999962747097 , 2.99999999859157 ] [ 1.9999999994207849 , 3.9999999995343387 , 2.9999999998614464 ] [ 1.9999999999182232 , 3.999999999941793 , 2.999999999978931 ] [ 1.9999999999907154 , 3.999999999992724 , 2.9999999999977414 ] [ 1.9999999999987457 , 3.9999999999990905 , 2.9999999999996803 ] [ 1.9999999999998526 , 3.9999999999998863 , 2.9999999999999636 ] [ 1.9999999999999807 , 3.999999999999986 , 2.999999999999995 ] [ 1.9999999999999978 , 3.9999999999999987 , 2.9999999999999996 ] [ 1.9999999999999996 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ]","title":"Eliminasi Gauss Seidel"},{"location":"Error Komnum/","text":"Galat/Error \u00b6 Pengertian \u00b6 Komputer Numerik dapat dikatakan sebagai penentuan error suatu perhitungan untuk mencapai nilai akurasi. Error disini adalah nilai yang menyebabkan nilai tersebut tidak tepat. Komputer Numerik bertujuan untuk menentukan suatu akurasi dari hasil perhitungan atau percobaan. Komputer numerik dapat dikatakan sebagai komputasi yang mengikuti suatu algoritma pendekatan untuk menyelesaikan suatu persoalan, yang dengan demikian besar kemungkinan di situ terkandung \u201ckesalahan\u201d. Beberapa sumber kesalahan pada komputasi numerik yaitu : Round off errors Kesalahan jenis ini terjadi akibat proses pembulatan dalam perhitungan. Secara umum, proses pembulatan ada 2 aturan yaitu : - Jika digit yang dibulatkan kurang dari 5, maka tidak terjadi pembulatan. - Sebaliknya, jika lebih dari 5, maka terjadi pembulatan yaitu dengan menambah satu. Truncation errors Kesalahan pemotongan terjadi ketika suatu rumus komputasi disederhanakan dengan cara membuang suku yang berderajat tinggi Inherent errors Terjadi akibat kekeliruan dalam menyalin data, salah membaca skala atau kesalahan karena kurangnya pengertian mengenai hukum-hukum fisik dari data yang diukur. Kesalahan ini sering terjadi karena faktor human error Definisi MacLaurin \u00b6 Suatu fungsi f(x) yang memiliki turunan , , , dan seterusnya yang kontinyu dalam interval dengan maka untuk disekitar yaitu , dapat diekspansi kedalam Deret Definisi Taylor. Berikut algoritma dari MacLaurin \u00b6 Dengan algoritma diatas kita dapat menyerderhanakannya sebagai berikut : Berikut contoh implementasi dari MacLaurin : f(x)= e 2x $$ f(x)\u22481+2x \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ Sekarang kita masukan misal x=0 $$ f(0)\u22481+2(0) \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ Jadi ketika x =0 maka hasil akan tetap 1 mekipun banyak suku dan literasi Listing Program \u00b6 Membuat program supaya dapat mengekspansi bilangan e 3x dengan nilai x=4 hingga nilai menjadi kurang dari 0,001 bisa dengan listing program sebagai berikut : import math coba = 1 a = 0 b = 1 x = int ( input ( \"masukkan x = \" )) while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 2 ** i ) * x ** i / math . factorial ( i ) for j in range ( b ): f_y += ( 2 ** j ) * x ** j / math . factorial ( j ) print ( \"suku ke \" , a , \"=\" , f_x ) print ( \"suku ke \" , b , \"=\" , f_y ) coba = f_y - f_x a += 1 b += 1 print ( \"selisih sukunya = \" , coba ) output : masukkan x = 1 suku ke 0 = 0 suku ke 1 = 1.0 selisih sukunya = 1.0 suku ke 1 = 1.0 suku ke 2 = 3.0 selisih sukunya = 2.0 suku ke 2 = 3.0 suku ke 3 = 5.0 selisih sukunya = 2.0 suku ke 3 = 5.0 suku ke 4 = 6.333333333333333 selisih sukunya = 1.333333333333333 suku ke 4 = 6.333333333333333 suku ke 5 = 7.0 selisih sukunya = 0.666666666666667 suku ke 5 = 7.0 suku ke 6 = 7.266666666666667 selisih sukunya = 0.2666666666666666 suku ke 6 = 7.266666666666667 suku ke 7 = 7.355555555555555 selisih sukunya = 0.08888888888888857 suku ke 7 = 7.355555555555555 suku ke 8 = 7.3809523809523805 selisih sukunya = 0.025396825396825307 suku ke 8 = 7.3809523809523805 suku ke 9 = 7.387301587301587 selisih sukunya = 0.006349206349206327 suku ke 9 = 7.387301587301587 suku ke 10 = 7.3887125220458545 selisih sukunya = 0.0014109347442676778 suku ke 10 = 7.3887125220458545 suku ke 11 = 7.388994708994708 selisih sukunya = 0.0002821869488531803 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Galat/Error"},{"location":"Error Komnum/#galaterror","text":"","title":"Galat/Error"},{"location":"Error Komnum/#pengertian","text":"Komputer Numerik dapat dikatakan sebagai penentuan error suatu perhitungan untuk mencapai nilai akurasi. Error disini adalah nilai yang menyebabkan nilai tersebut tidak tepat. Komputer Numerik bertujuan untuk menentukan suatu akurasi dari hasil perhitungan atau percobaan. Komputer numerik dapat dikatakan sebagai komputasi yang mengikuti suatu algoritma pendekatan untuk menyelesaikan suatu persoalan, yang dengan demikian besar kemungkinan di situ terkandung \u201ckesalahan\u201d. Beberapa sumber kesalahan pada komputasi numerik yaitu : Round off errors Kesalahan jenis ini terjadi akibat proses pembulatan dalam perhitungan. Secara umum, proses pembulatan ada 2 aturan yaitu : - Jika digit yang dibulatkan kurang dari 5, maka tidak terjadi pembulatan. - Sebaliknya, jika lebih dari 5, maka terjadi pembulatan yaitu dengan menambah satu. Truncation errors Kesalahan pemotongan terjadi ketika suatu rumus komputasi disederhanakan dengan cara membuang suku yang berderajat tinggi Inherent errors Terjadi akibat kekeliruan dalam menyalin data, salah membaca skala atau kesalahan karena kurangnya pengertian mengenai hukum-hukum fisik dari data yang diukur. Kesalahan ini sering terjadi karena faktor human error","title":"Pengertian"},{"location":"Error Komnum/#definisi-maclaurin","text":"Suatu fungsi f(x) yang memiliki turunan , , , dan seterusnya yang kontinyu dalam interval dengan maka untuk disekitar yaitu , dapat diekspansi kedalam Deret Definisi Taylor.","title":"Definisi MacLaurin"},{"location":"Error Komnum/#berikut-algoritma-dari-maclaurin","text":"Dengan algoritma diatas kita dapat menyerderhanakannya sebagai berikut : Berikut contoh implementasi dari MacLaurin : f(x)= e 2x $$ f(x)\u22481+2x \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ Sekarang kita masukan misal x=0 $$ f(0)\u22481+2(0) \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ Jadi ketika x =0 maka hasil akan tetap 1 mekipun banyak suku dan literasi","title":"Berikut algoritma dari MacLaurin"},{"location":"Error Komnum/#listing-program","text":"Membuat program supaya dapat mengekspansi bilangan e 3x dengan nilai x=4 hingga nilai menjadi kurang dari 0,001 bisa dengan listing program sebagai berikut : import math coba = 1 a = 0 b = 1 x = int ( input ( \"masukkan x = \" )) while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 2 ** i ) * x ** i / math . factorial ( i ) for j in range ( b ): f_y += ( 2 ** j ) * x ** j / math . factorial ( j ) print ( \"suku ke \" , a , \"=\" , f_x ) print ( \"suku ke \" , b , \"=\" , f_y ) coba = f_y - f_x a += 1 b += 1 print ( \"selisih sukunya = \" , coba ) output : masukkan x = 1 suku ke 0 = 0 suku ke 1 = 1.0 selisih sukunya = 1.0 suku ke 1 = 1.0 suku ke 2 = 3.0 selisih sukunya = 2.0 suku ke 2 = 3.0 suku ke 3 = 5.0 selisih sukunya = 2.0 suku ke 3 = 5.0 suku ke 4 = 6.333333333333333 selisih sukunya = 1.333333333333333 suku ke 4 = 6.333333333333333 suku ke 5 = 7.0 selisih sukunya = 0.666666666666667 suku ke 5 = 7.0 suku ke 6 = 7.266666666666667 selisih sukunya = 0.2666666666666666 suku ke 6 = 7.266666666666667 suku ke 7 = 7.355555555555555 selisih sukunya = 0.08888888888888857 suku ke 7 = 7.355555555555555 suku ke 8 = 7.3809523809523805 selisih sukunya = 0.025396825396825307 suku ke 8 = 7.3809523809523805 suku ke 9 = 7.387301587301587 selisih sukunya = 0.006349206349206327 suku ke 9 = 7.387301587301587 suku ke 10 = 7.3887125220458545 selisih sukunya = 0.0014109347442676778 suku ke 10 = 7.3887125220458545 suku ke 11 = 7.388994708994708 selisih sukunya = 0.0002821869488531803 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Listing Program"},{"location":"Mencari Akar/","text":"Mencari Akar dari Fungsi Transendental \u00b6 Solusi persamaan \ud835\udc53(\ud835\udc65)=0 terdapat di bidang sains, engineering dan aplikasi lainnya. Jika \ud835\udc53(\ud835\udc65) adalah sebuah polynomial dengan dua pangkat atau lebih, kita punya formula untuk menyelesaikannya. Tetapi, jika \ud835\udc53(\ud835\udc65) adalah fungsi transcendental (fungsi yang tidak bisa diekspresikan dengan fungsi aljabar, contoh: fungsi trigonometri, exponensial, atau inversi dari keduanya), kita belum memiliki formula untuk mendapatkan solusi. Saat berhadapan dengan persamaan ini, kita memiliki metoda seperti Bisection, Newton-Raphson, Secant dan metoda \u201csalah-posisi\u201d (biasa juga disebut sebagai metoda regula falsi). Metoda-metoda tersebut menyelesaikan persamaan transcendental dengan menggunakan teori persamaan, yaitu: jika \ud835\udc53(\ud835\udc65) kontinu di interval (a,b) dan jika f(a) dan f(b) berlawanan tanda, maka \ud835\udc53(\ud835\udc65)=0 setidaknya akan memiliki satu akar real antara a dan b. 1. Metode Bisection \u00b6 Misalkan kita punya persamaan \ud835\udc53(\ud835\udc65)=0 dimana akarnya berada di antara (a,b), dan \ud835\udc53(\ud835\udc65) adalah persamaan kontinu dan bisa berupa persamaan aljabar atau transcendental. Jika f(a) dan f(b) berlawanan tanda, maka \ud835\udc53(\ud835\udc65)=0 setidaknya akan memiliki satu akar real antara a dan b. Anggap \ud835\udc53(\ud835\udc4e) positif dan \ud835\udc53(\ud835\udc4f) negatif, yang mengimplikasikan bahwa setidaknya satu akar berada antara a dan b. Kita asumsikan bahwa akarnya adalah \ud835\udc650=(\ud835\udc4e+\ud835\udc4f)/2. Cek tanda \ud835\udc53(\ud835\udc650). Jika \ud835\udc53(\ud835\udc650) negatif, akarnya berada di antara a dan \ud835\udc650. Jika \ud835\udc53(\ud835\udc650) positif, maka akarnya berada di antara \ud835\udc650 dan b. Sehingga, hasilnya adalah salah satu diantara ini: \ud835\udc651=\ud835\udc4e+\ud835\udc6502 , atau \ud835\udc651=\ud835\udc650+\ud835\udc4f2 Jika \ud835\udc53(\ud835\udc651) negatif, akarnya berada di antara \ud835\udc650 dan \ud835\udc651, sehingga \ud835\udc652=(\ud835\udc650+\ud835\udc651)/2. Dan seterusnya, jika \ud835\udc53(\ud835\udc652) negatif, maka akarnya berada di antara \ud835\udc650 dan \ud835\udc652, dan \ud835\udc653=(\ud835\udc650+\ud835\udc652)/2 dan seterusnya. Ulangi proses \ud835\udc650,\ud835\udc651,\ud835\udc652,\u2026.sampai limit konvergensinya adalah akar dari persamaan tersebut. Langkah: 1. Cari a dan b dimana \ud835\udc53(\ud835\udc4e) dan \ud835\udc53(\ud835\udc4f) berlawanan tanda dengan metoda trial dan error 2. Asumsikan akar awal sebagai \ud835\udc650=(\ud835\udc4e+\ud835\udc4f)/2 3. Jika \ud835\udc53(\ud835\udc650) negatif, maka akarnya berada di antara a dan \ud835\udc650, dan buat akar berikutnya sebagai \ud835\udc651=\ud835\udc4e+\ud835\udc6502 4. Jika \ud835\udc53(\ud835\udc650) positif, maka akarnya berada di antara \ud835\udc650 dan b, dan buat akar berikutnya sebagai \ud835\udc651=\ud835\udc650+\ud835\udc4f2 5. Jika \ud835\udc53(\ud835\udc651) negatif, akarnya berada di antara \ud835\udc650 dan \ud835\udc651, dan buat akar berikutnya sebagai \ud835\udc652=(\ud835\udc650+\ud835\udc651)/2 6. Jika \ud835\udc53(\ud835\udc652) negatif, maka akarnya berada di antara \ud835\udc650 dan \ud835\udc652, dan buat akar berikutnya sebagai \ud835\udc653=(\ud835\udc650+\ud835\udc652)/2 7. Ulangi proses tersebut hingga dua angka berurutannya sama dan angka tersebut adalah akarnya Contoh Program : def bisection ( f , a , b , N ): '''Approximate solution of f(x)=0 on interval [a,b] by bisection method. Parameters ---------- f : function The function for which we are trying to approximate a solution f(x)=0. a,b : numbers The interval in which to search for a solution. The function returns None if f(a)*f(b) >= 0 since a solution is not guaranteed. N : (positive) integer The number of iterations to implement. Returns ------- x_N : number The midpoint of the Nth interval computed by the bisection method. The initial interval [a_0,b_0] is given by [a,b]. If f(m_n) == 0 for some midpoint m_n = (a_n + b_n)/2, then the function returns this solution. If all signs of values f(a_n), f(b_n) and f(m_n) are the same at any iteration, the bisection method fails and return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> bisection(f,1,2,25) 1.618033990263939 >>> f = lambda x: (2*x - 1)*(x - 3) >>> bisection(f,0,1,10) 0.5 ''' if f ( a ) * f ( b ) >= 0 : print ( \"Bisection method fails.\" ) return None a_n = a b_n = b for n in range ( 1 , N + 1 ): m_n = ( a_n + b_n ) / 2 f_m_n = f ( m_n ) if f ( a_n ) * f_m_n < 0 : a_n = a_n b_n = m_n print ( \"a = \" , a_n ) print ( \"b = \" , b_n ) elif f ( b_n ) * f_m_n < 0 : a_n = m_n b_n = b_n print ( \"a = \" , a_n ) print ( \"b = \" , b_n ) elif f_m_n == 0 : print ( \"Found exact solution.\" ) return m_n else : print ( \"Bisection method fails.\" ) return None return ( a_n + b_n ) / 2 f = lambda x : x ** 2 - 5 * x + 6 approx_phi = bisection ( f , 1 , 2.5 , 25 ) print ( \"x = \" , approx_phi ) ##error_bound = 2**(-26) ##print(error_bound) ##print(abs( (1 + 5**0.5)/2 - approx_phi) < error_bound) 2. Metode Newtoon - Raphson \u00b6 Misalkan kita punya persamaan \ud835\udc53(\ud835\udc65)=0 dimana akarnya berada antara (a,b), dan \ud835\udc53(\ud835\udc65) adalah persamaan kontinu dan bisa berupa persamaan aljabar atau transcendental. Jika f(a) dan f(b) berlawanan tanda, maka \ud835\udc53(\ud835\udc65)=0 setidaknya akan memiliki satu akar real antara a dan b. Anggap \ud835\udc53(\ud835\udc4e) positif dan \ud835\udc53(\ud835\udc4f) negatif, yang mengimplikasikan bahwa setidaknya satu akar berada antara a dan b. Kita asumsikan bahwa akarnya adalah a atau b, yang mana diantara \ud835\udc53(\ud835\udc4e) atau \ud835\udc53(\ud835\udc4f) yang nilainya lebih dekat ke nol. Angka tersebut diasumsikan sebagai akar pertama. Kemudian kita iterasi proses tersebut dengan menggunakan persamaan berikut sampai perhitungannya konvergen. Langkah: Cari a dan b dimana \ud835\udc53(\ud835\udc4e) dan \ud835\udc53(\ud835\udc4f) berlawanan tanda dengan metoda trial dan error Asumsikan akar awal adalah \ud835\udc4b0=\ud835\udc4e, jika nilai \ud835\udc53(\ud835\udc4e) mendekati nol atau \ud835\udc4b0=\ud835\udc4f jika nilai \ud835\udc53(\ud835\udc4f) mendekati nol Cari \ud835\udc4b1 dengan menggunakan persamaan \ud835\udc4b1=\ud835\udc4b0\u2212\ud835\udc53(\ud835\udc4b0)\ud835\udc53\u2032(\ud835\udc4b0) Cari \ud835\udc4b2 dengan menggunakan persamaan \ud835\udc4b2=\ud835\udc4b1\u2212\ud835\udc53(\ud835\udc4b1)\ud835\udc53\u2032(\ud835\udc4b1) Cari \ud835\udc4b3,\ud835\udc4b4,\u2026\ud835\udc4b\ud835\udc5b sampai nilai yang dihasilkan sama. Contoh Program : def newton ( f , Df , x0 , epsilon , max_iter ): '''Approximate solution of f(x)=0 by Newton's method. Parameters ---------- f : function Function for which we are searching for a solution f(x)=0. Df : function Derivative of f(x). x0 : number Initial guess for a solution f(x)=0. epsilon : number Stopping criteria is abs(f(x)) < epsilon. max_iter : integer Maximum number of iterations of Newton's method. Returns ------- xn : number Implement Newton's method: compute the linear approximation of f(x) at xn and find x intercept by the formula x = xn - f(xn)/Df(xn) Continue until abs(f(xn)) < epsilon and return xn. If Df(xn) == 0, return None. If the number of iterations exceeds max_iter, then return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> Df = lambda x: 2*x - 1 >>> newton(f,Df,1,1e-8,10) Found solution after 5 iterations. 1.618033988749989 ''' xn = x0 for n in range ( 0 , max_iter ): fxn = f ( xn ) if abs ( fxn ) < epsilon : print ( 'Found solution after' , n , 'iterations.' ) return xn Dfxn = Df ( xn ) if Dfxn == 0 : print ( 'Zero derivative. No solution found.' ) return None xn = xn - fxn / Dfxn print ( 'Exceeded maximum iterations. No solution found.' ) return None p = lambda x : x ** 3 - x ** 2 - 1 Dp = lambda x : 3 * x ** 2 - 2 * x approx = newton ( p , Dp , 1 , 1e-10 , 10 ) print ( approx ) ##f = lambda x: x**(1/3) ##Df = lambda x: (1/3)*x**(-2/3) ##approx = newton(f,Df,0.1,1e-2,100) 3. Metode Regula - Falsi \u00b6 Metode Regula Falsi adalah salah satu metode numerik yang digunakan untuk mencari akar dari suatu persamaan dengan memanfaatkan kemiringan dan selisih tinggi dari dari dua titik batas range. Sebenarnya metode ini hampir sama dengan Metode Biseksi, tapi titik pendekatan yang digunakan pada metode ini berbeda dengan Metode Biseksi. Rumus titik pendekatan tersebut adalah : Keterangan : a = X0 b = X1 m = X2 Metode regula falsi merupakan salah satu metode tertutup untuk menentukan solusi akar dari persamaan non linier. Berikut langkah penyelesaiannya : 1) Tentukan interval [X0, X1] yang memuat akar 2) Tentukan titik X2 dengan menarik garis lurus dari titik [X0, f (X0)] ke titik [X1, f (X1)] titik X2 adalah titik potong garis dengan sumbu X. X2 = X0 * f (X1) - X1 * f (X0) / f (X1) - f (X0) X2 = X1 - [ (X1 -X0) / f (X1) - f (X0) ] * f (X1) [P] X2 = X1 - P * f (X1) 3) Suatu kondisi bila : f (X0) * f (X2) < 0 Maka akar pada [X0, X2] , X2 = X1 f (X0) * f (X2) = 0 akar = X2 f (X0) * f (X2) > 0 Maka akar pada [X2, X1], X2 = X0 4) Pengulangan / iterasi mencari X2 dan interval baru dilakukan berdasarkan nilai toleransi | (X2 - X)1 / X1 | atau | (X2 - X0) / X0 | 5) Kelemahan : Hanya salah satu ujung titik interval ( X0 atau X1 ) yang bergerak menuju akar dan yang lain selalu tetap untuk setiap iterasi [ nilai bersifat mutlak ] Contoh Program : error = 0.01 a = 0 b = 2.1 def f ( x ): return x ** 2 - 5 * x + 6 def regulasi_falsi ( a , b ): i = 0 max_iter = 50 iteration = True while iteration and i < max_iter : if f ( a ) * f ( b ) < 0 : x = ( a * abs ( f ( b )) + b * abs ( f ( a ))) / ( abs ( f ( a )) + abs ( f ( b ))) if f ( a ) * f ( x ) < 0 : b = x print ( x ) if f ( x ) * f ( b ) < 0 : a = x print ( x ) if abs ( a - b ) < error : iteration = False else : i += 1 else : print ( 'tidak di temukan akar' ) print ( 'x =' , x ) regulasi_falsi ( a , b ) 4. Metode Secant \u00b6 Metode secant merupakan perbaikan dari metode regula-falsi dan newton raphson dimana kemiringan dua titik dinyatakan sacara diskrit, dengan mengambil bentuk garis lurus yang melalui satu titik. Tujuan metode secant adalah untuk menyelesaikan masalah yang terdapat pada metode Newton-Raphson yang terkadang sulit mendapatkan turunan pertama yaitu f\u2018 (x) . Fungsi metode secant adalah untuk menaksirkan akar dengan menggunakan diferensi daripada turunan untuk memperkirakan kemiringan/slope. f\u2019 (x) = ( f (xn) \u2013 f (xn-1)) / *(xn \u2013 xn-1) * xn+1 = xn \u2013 ( ( f(xn) (xn \u2013 xn-1)) / ( f(xn) -f(xn-1)* ) Algoritma Metode Secant : Definisikan fungsi f(x) Definisikan torelansi error (e) dan iterasi maksimum (n) Masukkan dua nilai pendekatan awal yang di antaranya terdapat akar yaitu x0 dan x1,sebaiknya gunakan metode tabel atau grafis untuk menjamin titik pendakatannya adalah titik pendekatan yang konvergensinya pada akar persamaan yang diharapkan. Hitung f(x0) dan f(x1) sebagai y0 dan y1 Untuk iterasi I = 1 s/d n atau | f(xn) | Xn+1 = Xn \u2013 Yn (Xn \u2013 Xn-1 / Yn \u2013 Yn-1) Akar persamaan adalah nilai x yang terakhir. Contoh Program : def secant ( f , a , b , N ): '''Approximate solution of f(x)=0 on interval [a,b] by the secant method. Parameters ---------- f : function The function for which we are trying to approximate a solution f(x)=0. a,b : numbers The interval in which to search for a solution. The function returns None if f(a)*f(b) >= 0 since a solution is not guaranteed. N : (positive) integer The number of iterations to implement. Returns ------- m_N : number The x intercept of the secant line on the the Nth interval m_n = a_n - f(a_n)*(b_n - a_n)/(f(b_n) - f(a_n)) The initial interval [a_0,b_0] is given by [a,b]. If f(m_n) == 0 for some intercept m_n then the function returns this solution. If all signs of values f(a_n), f(b_n) and f(m_n) are the same at any iterations, the secant method fails and return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> secant(f,1,2,5) 1.6180257510729614 ''' if f ( a ) * f ( b ) >= 0 : print ( \"Secant method fails.\" ) return None a_n = a b_n = b for n in range ( 1 , N + 1 ): m_n = a_n - f ( a_n ) * ( b_n - a_n ) / ( f ( b_n ) - f ( a_n )) f_m_n = f ( m_n ) if f ( a_n ) * f_m_n < 0 : a_n = a_n b_n = m_n elif f ( b_n ) * f_m_n < 0 : a_n = m_n b_n = b_n elif f_m_n == 0 : print ( \"Found exact solution.\" ) return m_n else : print ( \"Secant method fails.\" ) return None return a_n - f ( a_n ) * ( b_n - a_n ) / ( f ( b_n ) - f ( a_n )) p = lambda x : x ** 2 - 5 * x + 6 print ( p ( 1 )) print ( p ( 2 )) approx = secant ( p , 1 , 2.5 , 25 ) print ( approx )","title":"Mencari Akar"},{"location":"Mencari Akar/#mencari-akar-dari-fungsi-transendental","text":"Solusi persamaan \ud835\udc53(\ud835\udc65)=0 terdapat di bidang sains, engineering dan aplikasi lainnya. Jika \ud835\udc53(\ud835\udc65) adalah sebuah polynomial dengan dua pangkat atau lebih, kita punya formula untuk menyelesaikannya. Tetapi, jika \ud835\udc53(\ud835\udc65) adalah fungsi transcendental (fungsi yang tidak bisa diekspresikan dengan fungsi aljabar, contoh: fungsi trigonometri, exponensial, atau inversi dari keduanya), kita belum memiliki formula untuk mendapatkan solusi. Saat berhadapan dengan persamaan ini, kita memiliki metoda seperti Bisection, Newton-Raphson, Secant dan metoda \u201csalah-posisi\u201d (biasa juga disebut sebagai metoda regula falsi). Metoda-metoda tersebut menyelesaikan persamaan transcendental dengan menggunakan teori persamaan, yaitu: jika \ud835\udc53(\ud835\udc65) kontinu di interval (a,b) dan jika f(a) dan f(b) berlawanan tanda, maka \ud835\udc53(\ud835\udc65)=0 setidaknya akan memiliki satu akar real antara a dan b.","title":"Mencari Akar dari Fungsi Transendental"},{"location":"Mencari Akar/#1-metode-bisection","text":"Misalkan kita punya persamaan \ud835\udc53(\ud835\udc65)=0 dimana akarnya berada di antara (a,b), dan \ud835\udc53(\ud835\udc65) adalah persamaan kontinu dan bisa berupa persamaan aljabar atau transcendental. Jika f(a) dan f(b) berlawanan tanda, maka \ud835\udc53(\ud835\udc65)=0 setidaknya akan memiliki satu akar real antara a dan b. Anggap \ud835\udc53(\ud835\udc4e) positif dan \ud835\udc53(\ud835\udc4f) negatif, yang mengimplikasikan bahwa setidaknya satu akar berada antara a dan b. Kita asumsikan bahwa akarnya adalah \ud835\udc650=(\ud835\udc4e+\ud835\udc4f)/2. Cek tanda \ud835\udc53(\ud835\udc650). Jika \ud835\udc53(\ud835\udc650) negatif, akarnya berada di antara a dan \ud835\udc650. Jika \ud835\udc53(\ud835\udc650) positif, maka akarnya berada di antara \ud835\udc650 dan b. Sehingga, hasilnya adalah salah satu diantara ini: \ud835\udc651=\ud835\udc4e+\ud835\udc6502 , atau \ud835\udc651=\ud835\udc650+\ud835\udc4f2 Jika \ud835\udc53(\ud835\udc651) negatif, akarnya berada di antara \ud835\udc650 dan \ud835\udc651, sehingga \ud835\udc652=(\ud835\udc650+\ud835\udc651)/2. Dan seterusnya, jika \ud835\udc53(\ud835\udc652) negatif, maka akarnya berada di antara \ud835\udc650 dan \ud835\udc652, dan \ud835\udc653=(\ud835\udc650+\ud835\udc652)/2 dan seterusnya. Ulangi proses \ud835\udc650,\ud835\udc651,\ud835\udc652,\u2026.sampai limit konvergensinya adalah akar dari persamaan tersebut. Langkah: 1. Cari a dan b dimana \ud835\udc53(\ud835\udc4e) dan \ud835\udc53(\ud835\udc4f) berlawanan tanda dengan metoda trial dan error 2. Asumsikan akar awal sebagai \ud835\udc650=(\ud835\udc4e+\ud835\udc4f)/2 3. Jika \ud835\udc53(\ud835\udc650) negatif, maka akarnya berada di antara a dan \ud835\udc650, dan buat akar berikutnya sebagai \ud835\udc651=\ud835\udc4e+\ud835\udc6502 4. Jika \ud835\udc53(\ud835\udc650) positif, maka akarnya berada di antara \ud835\udc650 dan b, dan buat akar berikutnya sebagai \ud835\udc651=\ud835\udc650+\ud835\udc4f2 5. Jika \ud835\udc53(\ud835\udc651) negatif, akarnya berada di antara \ud835\udc650 dan \ud835\udc651, dan buat akar berikutnya sebagai \ud835\udc652=(\ud835\udc650+\ud835\udc651)/2 6. Jika \ud835\udc53(\ud835\udc652) negatif, maka akarnya berada di antara \ud835\udc650 dan \ud835\udc652, dan buat akar berikutnya sebagai \ud835\udc653=(\ud835\udc650+\ud835\udc652)/2 7. Ulangi proses tersebut hingga dua angka berurutannya sama dan angka tersebut adalah akarnya Contoh Program : def bisection ( f , a , b , N ): '''Approximate solution of f(x)=0 on interval [a,b] by bisection method. Parameters ---------- f : function The function for which we are trying to approximate a solution f(x)=0. a,b : numbers The interval in which to search for a solution. The function returns None if f(a)*f(b) >= 0 since a solution is not guaranteed. N : (positive) integer The number of iterations to implement. Returns ------- x_N : number The midpoint of the Nth interval computed by the bisection method. The initial interval [a_0,b_0] is given by [a,b]. If f(m_n) == 0 for some midpoint m_n = (a_n + b_n)/2, then the function returns this solution. If all signs of values f(a_n), f(b_n) and f(m_n) are the same at any iteration, the bisection method fails and return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> bisection(f,1,2,25) 1.618033990263939 >>> f = lambda x: (2*x - 1)*(x - 3) >>> bisection(f,0,1,10) 0.5 ''' if f ( a ) * f ( b ) >= 0 : print ( \"Bisection method fails.\" ) return None a_n = a b_n = b for n in range ( 1 , N + 1 ): m_n = ( a_n + b_n ) / 2 f_m_n = f ( m_n ) if f ( a_n ) * f_m_n < 0 : a_n = a_n b_n = m_n print ( \"a = \" , a_n ) print ( \"b = \" , b_n ) elif f ( b_n ) * f_m_n < 0 : a_n = m_n b_n = b_n print ( \"a = \" , a_n ) print ( \"b = \" , b_n ) elif f_m_n == 0 : print ( \"Found exact solution.\" ) return m_n else : print ( \"Bisection method fails.\" ) return None return ( a_n + b_n ) / 2 f = lambda x : x ** 2 - 5 * x + 6 approx_phi = bisection ( f , 1 , 2.5 , 25 ) print ( \"x = \" , approx_phi ) ##error_bound = 2**(-26) ##print(error_bound) ##print(abs( (1 + 5**0.5)/2 - approx_phi) < error_bound)","title":"1. Metode Bisection"},{"location":"Mencari Akar/#2-metode-newtoon-raphson","text":"Misalkan kita punya persamaan \ud835\udc53(\ud835\udc65)=0 dimana akarnya berada antara (a,b), dan \ud835\udc53(\ud835\udc65) adalah persamaan kontinu dan bisa berupa persamaan aljabar atau transcendental. Jika f(a) dan f(b) berlawanan tanda, maka \ud835\udc53(\ud835\udc65)=0 setidaknya akan memiliki satu akar real antara a dan b. Anggap \ud835\udc53(\ud835\udc4e) positif dan \ud835\udc53(\ud835\udc4f) negatif, yang mengimplikasikan bahwa setidaknya satu akar berada antara a dan b. Kita asumsikan bahwa akarnya adalah a atau b, yang mana diantara \ud835\udc53(\ud835\udc4e) atau \ud835\udc53(\ud835\udc4f) yang nilainya lebih dekat ke nol. Angka tersebut diasumsikan sebagai akar pertama. Kemudian kita iterasi proses tersebut dengan menggunakan persamaan berikut sampai perhitungannya konvergen. Langkah: Cari a dan b dimana \ud835\udc53(\ud835\udc4e) dan \ud835\udc53(\ud835\udc4f) berlawanan tanda dengan metoda trial dan error Asumsikan akar awal adalah \ud835\udc4b0=\ud835\udc4e, jika nilai \ud835\udc53(\ud835\udc4e) mendekati nol atau \ud835\udc4b0=\ud835\udc4f jika nilai \ud835\udc53(\ud835\udc4f) mendekati nol Cari \ud835\udc4b1 dengan menggunakan persamaan \ud835\udc4b1=\ud835\udc4b0\u2212\ud835\udc53(\ud835\udc4b0)\ud835\udc53\u2032(\ud835\udc4b0) Cari \ud835\udc4b2 dengan menggunakan persamaan \ud835\udc4b2=\ud835\udc4b1\u2212\ud835\udc53(\ud835\udc4b1)\ud835\udc53\u2032(\ud835\udc4b1) Cari \ud835\udc4b3,\ud835\udc4b4,\u2026\ud835\udc4b\ud835\udc5b sampai nilai yang dihasilkan sama. Contoh Program : def newton ( f , Df , x0 , epsilon , max_iter ): '''Approximate solution of f(x)=0 by Newton's method. Parameters ---------- f : function Function for which we are searching for a solution f(x)=0. Df : function Derivative of f(x). x0 : number Initial guess for a solution f(x)=0. epsilon : number Stopping criteria is abs(f(x)) < epsilon. max_iter : integer Maximum number of iterations of Newton's method. Returns ------- xn : number Implement Newton's method: compute the linear approximation of f(x) at xn and find x intercept by the formula x = xn - f(xn)/Df(xn) Continue until abs(f(xn)) < epsilon and return xn. If Df(xn) == 0, return None. If the number of iterations exceeds max_iter, then return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> Df = lambda x: 2*x - 1 >>> newton(f,Df,1,1e-8,10) Found solution after 5 iterations. 1.618033988749989 ''' xn = x0 for n in range ( 0 , max_iter ): fxn = f ( xn ) if abs ( fxn ) < epsilon : print ( 'Found solution after' , n , 'iterations.' ) return xn Dfxn = Df ( xn ) if Dfxn == 0 : print ( 'Zero derivative. No solution found.' ) return None xn = xn - fxn / Dfxn print ( 'Exceeded maximum iterations. No solution found.' ) return None p = lambda x : x ** 3 - x ** 2 - 1 Dp = lambda x : 3 * x ** 2 - 2 * x approx = newton ( p , Dp , 1 , 1e-10 , 10 ) print ( approx ) ##f = lambda x: x**(1/3) ##Df = lambda x: (1/3)*x**(-2/3) ##approx = newton(f,Df,0.1,1e-2,100)","title":"2. Metode Newtoon - Raphson"},{"location":"Mencari Akar/#3-metode-regula-falsi","text":"Metode Regula Falsi adalah salah satu metode numerik yang digunakan untuk mencari akar dari suatu persamaan dengan memanfaatkan kemiringan dan selisih tinggi dari dari dua titik batas range. Sebenarnya metode ini hampir sama dengan Metode Biseksi, tapi titik pendekatan yang digunakan pada metode ini berbeda dengan Metode Biseksi. Rumus titik pendekatan tersebut adalah : Keterangan : a = X0 b = X1 m = X2 Metode regula falsi merupakan salah satu metode tertutup untuk menentukan solusi akar dari persamaan non linier. Berikut langkah penyelesaiannya : 1) Tentukan interval [X0, X1] yang memuat akar 2) Tentukan titik X2 dengan menarik garis lurus dari titik [X0, f (X0)] ke titik [X1, f (X1)] titik X2 adalah titik potong garis dengan sumbu X. X2 = X0 * f (X1) - X1 * f (X0) / f (X1) - f (X0) X2 = X1 - [ (X1 -X0) / f (X1) - f (X0) ] * f (X1) [P] X2 = X1 - P * f (X1) 3) Suatu kondisi bila : f (X0) * f (X2) < 0 Maka akar pada [X0, X2] , X2 = X1 f (X0) * f (X2) = 0 akar = X2 f (X0) * f (X2) > 0 Maka akar pada [X2, X1], X2 = X0 4) Pengulangan / iterasi mencari X2 dan interval baru dilakukan berdasarkan nilai toleransi | (X2 - X)1 / X1 | atau | (X2 - X0) / X0 | 5) Kelemahan : Hanya salah satu ujung titik interval ( X0 atau X1 ) yang bergerak menuju akar dan yang lain selalu tetap untuk setiap iterasi [ nilai bersifat mutlak ] Contoh Program : error = 0.01 a = 0 b = 2.1 def f ( x ): return x ** 2 - 5 * x + 6 def regulasi_falsi ( a , b ): i = 0 max_iter = 50 iteration = True while iteration and i < max_iter : if f ( a ) * f ( b ) < 0 : x = ( a * abs ( f ( b )) + b * abs ( f ( a ))) / ( abs ( f ( a )) + abs ( f ( b ))) if f ( a ) * f ( x ) < 0 : b = x print ( x ) if f ( x ) * f ( b ) < 0 : a = x print ( x ) if abs ( a - b ) < error : iteration = False else : i += 1 else : print ( 'tidak di temukan akar' ) print ( 'x =' , x ) regulasi_falsi ( a , b )","title":"3. Metode Regula - Falsi"},{"location":"Mencari Akar/#4-metode-secant","text":"Metode secant merupakan perbaikan dari metode regula-falsi dan newton raphson dimana kemiringan dua titik dinyatakan sacara diskrit, dengan mengambil bentuk garis lurus yang melalui satu titik. Tujuan metode secant adalah untuk menyelesaikan masalah yang terdapat pada metode Newton-Raphson yang terkadang sulit mendapatkan turunan pertama yaitu f\u2018 (x) . Fungsi metode secant adalah untuk menaksirkan akar dengan menggunakan diferensi daripada turunan untuk memperkirakan kemiringan/slope. f\u2019 (x) = ( f (xn) \u2013 f (xn-1)) / *(xn \u2013 xn-1) * xn+1 = xn \u2013 ( ( f(xn) (xn \u2013 xn-1)) / ( f(xn) -f(xn-1)* ) Algoritma Metode Secant : Definisikan fungsi f(x) Definisikan torelansi error (e) dan iterasi maksimum (n) Masukkan dua nilai pendekatan awal yang di antaranya terdapat akar yaitu x0 dan x1,sebaiknya gunakan metode tabel atau grafis untuk menjamin titik pendakatannya adalah titik pendekatan yang konvergensinya pada akar persamaan yang diharapkan. Hitung f(x0) dan f(x1) sebagai y0 dan y1 Untuk iterasi I = 1 s/d n atau | f(xn) | Xn+1 = Xn \u2013 Yn (Xn \u2013 Xn-1 / Yn \u2013 Yn-1) Akar persamaan adalah nilai x yang terakhir. Contoh Program : def secant ( f , a , b , N ): '''Approximate solution of f(x)=0 on interval [a,b] by the secant method. Parameters ---------- f : function The function for which we are trying to approximate a solution f(x)=0. a,b : numbers The interval in which to search for a solution. The function returns None if f(a)*f(b) >= 0 since a solution is not guaranteed. N : (positive) integer The number of iterations to implement. Returns ------- m_N : number The x intercept of the secant line on the the Nth interval m_n = a_n - f(a_n)*(b_n - a_n)/(f(b_n) - f(a_n)) The initial interval [a_0,b_0] is given by [a,b]. If f(m_n) == 0 for some intercept m_n then the function returns this solution. If all signs of values f(a_n), f(b_n) and f(m_n) are the same at any iterations, the secant method fails and return None. Examples -------- >>> f = lambda x: x**2 - x - 1 >>> secant(f,1,2,5) 1.6180257510729614 ''' if f ( a ) * f ( b ) >= 0 : print ( \"Secant method fails.\" ) return None a_n = a b_n = b for n in range ( 1 , N + 1 ): m_n = a_n - f ( a_n ) * ( b_n - a_n ) / ( f ( b_n ) - f ( a_n )) f_m_n = f ( m_n ) if f ( a_n ) * f_m_n < 0 : a_n = a_n b_n = m_n elif f ( b_n ) * f_m_n < 0 : a_n = m_n b_n = b_n elif f_m_n == 0 : print ( \"Found exact solution.\" ) return m_n else : print ( \"Secant method fails.\" ) return None return a_n - f ( a_n ) * ( b_n - a_n ) / ( f ( b_n ) - f ( a_n )) p = lambda x : x ** 2 - 5 * x + 6 print ( p ( 1 )) print ( p ( 2 )) approx = secant ( p , 1 , 2.5 , 25 ) print ( approx )","title":"4. Metode Secant"},{"location":"Mengukur Jarak/","text":"Mengukur Jarak Data \u00b6 A. Mengukur Jarak Tipe Numerik \u00b6 Salah satu tantangan era saat ini adalah datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Algoritma seperti Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan. Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaitu v1, v2 menyatakandua vektor yang menyatakan v1=x1, x2,...,xn, v2=y1,y2,...,yn, dimana xi, yi disebut attribut. Ada beberapa ukuran similaritas data ukuran jarak, diantaranya : 1. Minkowski Distance \u00b6 Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ dimana m adalah bilangan riel positif dan xi dan yi adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. 2. Manhattan Distance \u00b6 Manhattan distance adalah kasus khusus dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. Bila ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ 3. Euclidean Distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkinan memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. 4. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adalah versi modikfikasi dari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x, y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ 5. Weighted Euclidean Distance \u00b6 Jika berdasarkan tingkatan penting dari masing-masing atribut ditentukan, maka Weighted Euclidean Distance adalah modifikisasi lain dari jarak Euclidean Distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana wi adalah bobot yang diberikan pada atribut ke i. 6. Chord Distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana \u2225x\u22252 adalah $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$ 7. Mahalanobis Distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ dimana S adalah matrik covariance data. 8. Cosine Measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan $$ Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } $$ dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn) didefinisikan dengan $$ |y|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } $$ 9. Pearson Correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara dua bentuk pola expresi gen. Pearson correlation didefinisikan dengan $$ Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } $$ The Pearson Correlation kelemahannya adalah sensitif terhadap outlier B. Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Atribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? Satu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 dimana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, dimana p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, dimana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i dan j dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient C. Mengukur Jarak Tipe categorical \u00b6 1. Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana n adalah banyaknya atribut, ai(x) dan ai(y) adalah nilai atribut ke i yaitu Ai dari masing masing objek x dan y, \u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. 2. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana C adalah banyaknya kelas, P(c|ai(x)) adalah probabilitas bersyarat dimana kelas x adalah c dari atribut Ai, yang memiliki nilai ai(x), P(c|ai(y)) adalah probabilitas bersyarat dimana kelas y adalah c dengan atribut Ai memiliki nilai ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ dimana probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes 3. Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ D. Mengukur Jarak Tipe Ordinal \u00b6 Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f yang memiliki Mf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f untuk objek ke-i adalah xif, dan ff memiliki Mf status urutan , mewakili peringkat 1,..,Mf Ganti setiap xif dengan peringkatnya, rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi E. Menghitung Jarak Tipe Campuran \u00b6 Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0]. Misalkan data berisi atribut p tipe campuran. Ketidaksamaan (disimilarity) antara objek i dan j dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Mengukur Jarak"},{"location":"Mengukur Jarak/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak/#a-mengukur-jarak-tipe-numerik","text":"Salah satu tantangan era saat ini adalah datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Algoritma seperti Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan. Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaitu v1, v2 menyatakandua vektor yang menyatakan v1=x1, x2,...,xn, v2=y1,y2,...,yn, dimana xi, yi disebut attribut. Ada beberapa ukuran similaritas data ukuran jarak, diantaranya :","title":"A. Mengukur Jarak Tipe Numerik"},{"location":"Mengukur Jarak/#1-minkowski-distance","text":"Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ dimana m adalah bilangan riel positif dan xi dan yi adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"1. Minkowski Distance"},{"location":"Mengukur Jarak/#2-manhattan-distance","text":"Manhattan distance adalah kasus khusus dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. Bila ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$","title":"2. Manhattan Distance"},{"location":"Mengukur Jarak/#3-euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkinan memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"3. Euclidean Distance"},{"location":"Mengukur Jarak/#4-average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adalah versi modikfikasi dari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x, y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$","title":"4. Average Distance"},{"location":"Mengukur Jarak/#5-weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing-masing atribut ditentukan, maka Weighted Euclidean Distance adalah modifikisasi lain dari jarak Euclidean Distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana wi adalah bobot yang diberikan pada atribut ke i.","title":"5. Weighted Euclidean Distance"},{"location":"Mengukur Jarak/#6-chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana \u2225x\u22252 adalah $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$","title":"6. Chord Distance"},{"location":"Mengukur Jarak/#7-mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ dimana S adalah matrik covariance data.","title":"7. Mahalanobis Distance"},{"location":"Mengukur Jarak/#8-cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan $$ Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } $$ dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn) didefinisikan dengan $$ |y|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } $$","title":"8. Cosine Measure"},{"location":"Mengukur Jarak/#9-pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara dua bentuk pola expresi gen. Pearson correlation didefinisikan dengan $$ Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } $$ The Pearson Correlation kelemahannya adalah sensitif terhadap outlier","title":"9. Pearson Correlation"},{"location":"Mengukur Jarak/#b-mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Atribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? Satu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 dimana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, dimana p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antar i dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, dimana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i dan j dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient","title":"B. Mengukur Jarak Atribut Binary"},{"location":"Mengukur Jarak/#c-mengukur-jarak-tipe-categorical","text":"","title":"C. Mengukur Jarak Tipe categorical"},{"location":"Mengukur Jarak/#1-overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana n adalah banyaknya atribut, ai(x) dan ai(y) adalah nilai atribut ke i yaitu Ai dari masing masing objek x dan y, \u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"1. Overlay Metric"},{"location":"Mengukur Jarak/#2-value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana C adalah banyaknya kelas, P(c|ai(x)) adalah probabilitas bersyarat dimana kelas x adalah c dari atribut Ai, yang memiliki nilai ai(x), P(c|ai(y)) adalah probabilitas bersyarat dimana kelas y adalah c dengan atribut Ai memiliki nilai ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ dimana probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes","title":"2. Value Difference Metric (VDM)"},{"location":"Mengukur Jarak/#3-minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"3. Minimum Risk Metric (MRM)"},{"location":"Mengukur Jarak/#d-mengukur-jarak-tipe-ordinal","text":"Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f yang memiliki Mf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f untuk objek ke-i adalah xif, dan ff memiliki Mf status urutan , mewakili peringkat 1,..,Mf Ganti setiap xif dengan peringkatnya, rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi","title":"D. Mengukur Jarak Tipe Ordinal"},{"location":"Mengukur Jarak/#e-menghitung-jarak-tipe-campuran","text":"Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0]. Misalkan data berisi atribut p tipe campuran. Ketidaksamaan (disimilarity) antara objek i dan j dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"E. Menghitung Jarak Tipe Campuran"},{"location":"Missing V/","text":"Missing Values \u00b6 Pengertian \u00b6 Missing value merupakan tidak tersedianya informasi pada suatu objek (kasus) yang terjadi karena informasi tersebut tidak diberikan, sulit dicari, atau memang tidak ada. Missing value sebernarnya tidak menjadi masalah bagi keseluruhan data, apalagi jika jumlahnya sedikit, misal hanya 1 % dari seluruh data. Namun jika persentase data yang hilang tersebut cukup besar, maka perlu dilakukan pengujian apakah data yang mengandung banyak missing tersebut masih layak diproses lebih lanjut ataukah tidak. Missing Values using KNN \u00b6 KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan. Kalibrasi Parameter KNN \u00b6 Jumlah tetangga yang harus dicari \u00b6 Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan. Metode agregasi untuk digunakan \u00b6 Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal Normalisasi data \u00b6 Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan. Atribut numerik jarak \u00b6 Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...). Atribut kategorikal jarak \u00b6 tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya Contoh Script Missing Values \u00b6 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0 Sumber : https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/","title":"Missing Values"},{"location":"Missing V/#missing-values","text":"","title":"Missing Values"},{"location":"Missing V/#pengertian","text":"Missing value merupakan tidak tersedianya informasi pada suatu objek (kasus) yang terjadi karena informasi tersebut tidak diberikan, sulit dicari, atau memang tidak ada. Missing value sebernarnya tidak menjadi masalah bagi keseluruhan data, apalagi jika jumlahnya sedikit, misal hanya 1 % dari seluruh data. Namun jika persentase data yang hilang tersebut cukup besar, maka perlu dilakukan pengujian apakah data yang mengandung banyak missing tersebut masih layak diproses lebih lanjut ataukah tidak.","title":"Pengertian"},{"location":"Missing V/#missing-values-using-knn","text":"KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan.","title":"Missing Values using KNN"},{"location":"Missing V/#kalibrasi-parameter-knn","text":"","title":"Kalibrasi Parameter KNN"},{"location":"Missing V/#jumlah-tetangga-yang-harus-dicari","text":"Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan.","title":"Jumlah tetangga yang harus dicari"},{"location":"Missing V/#metode-agregasi-untuk-digunakan","text":"Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal","title":"Metode agregasi untuk digunakan"},{"location":"Missing V/#normalisasi-data","text":"Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan.","title":"Normalisasi data"},{"location":"Missing V/#atribut-numerik-jarak","text":"Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...).","title":"Atribut numerik jarak"},{"location":"Missing V/#atribut-kategorikal-jarak","text":"tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya","title":"Atribut kategorikal jarak"},{"location":"Missing V/#contoh-script-missing-values","text":"# importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0 Sumber : https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/","title":"Contoh Script Missing Values"},{"location":"Pohon Keputusan/","text":"Pohon Keputusan \u00b6 Pengertian \u00b6 Pohon Keputusan (Decision Tree) adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Pohon Keputusan merupakan salah satu metode klasifikasi yang paling populer karena mudah untuk diinterpretasi oleh manusia. Pohon yang dalam analisis pemecahan masalah pengambilan keputusan adalah pemetaan mengenai alternatif-alternatif pemecahan masalah yang dapat diambil dari masalah tersebut. Konsep Pohon Keputusan \u00b6 Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Data dalam pohon keputusan biasanya dinyatakan dalam bentuk tabel dengan atribut dan record. Atribut menyatakan suatu parameter yang dibuat sebagai kriteria dalam pembentukan tree. Pohon keputusan merupakan himpunan aturan IF\u2026THEN. Setiap path dalam tree dihubungkan dengan sebuah aturan, di mana premis terdiri atas sekumpulan node-node yang ditemui, dan kesimpulan dari aturam terdiri atas kelas yang terhubung dengan leaf dari path. Bagian awal dari pohon keputusan ini adalah titik akar (root), sedangkan setiap cabang dari pohon keputusan merupakan pembagian berdasarkan hasil uji, dan titik akhir (leaf) merupakan pembagian kelas yang dihasilkan. Manfaat Pohon Keputusan \u00b6 Proses pada pohon keputusan adalah mengubah bentuk data (tabel) menjadi model pohon, mengubah model pohon menjadi rule, dan menyederhanakan rule. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk mem-break down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan mempunyai 3 tipe simpul \u00b6 Simpul akar, dimana tidak memiliki cabang yang masuk dan memiliki cabang lebih dari satu, terkadang tidak memiliki cabang sama sekali. Simpul ini biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. Simpul internal, dimana hanya memiliki 1 cabang yang masuk, dan memiliki lebih dari 1 cabang yang keluar. Simpul daun, atau simpul akhir dimana hanya memiliki 1 cabang yang masuk, dan tidak memiliki cabang sama sekali dan menandai bahwa simpul tersebut merupakan label kelas. Kelebihan Pohon Keputusan \u00b6 Tidak memerlukan biaya yang mahal saat membangun algoritma. Mudah untuk diinterpetasikan. Mudah mengintegrasikan dengan sistem basis data. Memiliki nilai ketelitian yang lebih baik. Dapat menemukan hubungan tak terduga dan suatu data. Dapat menggunakan data pasti/mutlak atau data kontinu. Mengakomodasi data yang hilang. Kekurangan pohon keputusan \u00b6 Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain. Cara Membuat Decision Tree \u00b6 Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen ( kolom , data ): kelas = [] for i in range ( len ( data )): if data . values . tolist ()[ i ][ kolom ] not in kelas : kelas . append ( data . values . tolist ()[ i ][ kolom ]) return kelas kelas = banyak_elemen ( df . shape [ 1 ] - 1 , df ) outlook = banyak_elemen ( df . shape [ 1 ] - 5 , df ) temp = banyak_elemen ( df . shape [ 1 ] - 4 , df ) humidity = banyak_elemen ( df . shape [ 1 ] - 3 , df ) windy = banyak_elemen ( df . shape [ 1 ] - 2 , df ) print ( kelas , outlook , temp , humidity , windy ) [ 'no' , 'yes' ] [ 'sunny' , 'overcast' , 'rainy' ] [ 'hot' , 'mild' , 'cool' ] [ 'high' , 'normal' ] [ False , True ] Fungsi count Kelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas ( kelas , kolomKelas , data ): hasil = [] for x in range ( len ( kelas )): hasil . append ( 0 ) for i in range ( len ( data )): for j in range ( len ( kelas )): if data . values . tolist ()[ i ][ kolomKelas ] == kelas [ j ]: hasil [ j ] += 1 return hasil pKelas = countvKelas ( kelas , df . shape [ 1 ] - 1 , df ) pKelas [ 5 , 9 ] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy ( T ): hasil = 0 jumlah = 0 for y in T : jumlah += y for z in range ( len ( T )): if jumlah != 0 : T [ z ] = T [ z ] / jumlah for i in T : if i != 0 : hasil -= i * math . log ( i , 2 ) return hasil def e_list ( atribut , n ): temp = [] tx = t_list ( atribut , n ) for i in range ( len ( atribut )): ent = entropy ( tx [ i ]) temp . append ( ent ) return temp tOutlook = t_list ( outlook , 5 ) tTemp = t_list ( temp , 4 ) tHum = t_list ( humidity , 3 ) tWin = t_list ( windy , 2 ) print ( \"Sunny, Overcast, Rainy\" , eOutlook ) print ( \"Hot, Mild, Cold\" , eTemp ) print ( \"High, Normal\" , eHum ) print ( \"False, True\" , eWin ) Sunny , Overcast , Rainy [ 0.9709505944546686 , 0.0 , 0.9709505944546686 ] Hot , Mild , Cold [ 1.0 , 0.9182958340544896 , 0.8112781244591328 ] High , Normal [ 0.9852281360342516 , 0.5916727785823275 ] False , True [ 0.8112781244591328 , 1.0 ] berikut contoh data yang akan di rubah menjadi decision tree 0 1 2 3 4 0 CUSTOMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUXURY EXTRA LARGE C1 15 15 F LUXURY SMALL C1 16 16 F LUXURY SMALL C1 17 17 F LUXURY MEDIUM C1 18 18 F LUXURY MEDIUM C1 19 19 F LUXURY MEDIUM C1 20 20 F LUXURY LARGE C1 pertama mencari entropy(s) dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"Pohon Keputusan"},{"location":"Pohon Keputusan/#pohon-keputusan","text":"","title":"Pohon Keputusan"},{"location":"Pohon Keputusan/#pengertian","text":"Pohon Keputusan (Decision Tree) adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Pohon Keputusan merupakan salah satu metode klasifikasi yang paling populer karena mudah untuk diinterpretasi oleh manusia. Pohon yang dalam analisis pemecahan masalah pengambilan keputusan adalah pemetaan mengenai alternatif-alternatif pemecahan masalah yang dapat diambil dari masalah tersebut.","title":"Pengertian"},{"location":"Pohon Keputusan/#konsep-pohon-keputusan","text":"Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Data dalam pohon keputusan biasanya dinyatakan dalam bentuk tabel dengan atribut dan record. Atribut menyatakan suatu parameter yang dibuat sebagai kriteria dalam pembentukan tree. Pohon keputusan merupakan himpunan aturan IF\u2026THEN. Setiap path dalam tree dihubungkan dengan sebuah aturan, di mana premis terdiri atas sekumpulan node-node yang ditemui, dan kesimpulan dari aturam terdiri atas kelas yang terhubung dengan leaf dari path. Bagian awal dari pohon keputusan ini adalah titik akar (root), sedangkan setiap cabang dari pohon keputusan merupakan pembagian berdasarkan hasil uji, dan titik akhir (leaf) merupakan pembagian kelas yang dihasilkan.","title":"Konsep Pohon Keputusan"},{"location":"Pohon Keputusan/#manfaat-pohon-keputusan","text":"Proses pada pohon keputusan adalah mengubah bentuk data (tabel) menjadi model pohon, mengubah model pohon menjadi rule, dan menyederhanakan rule. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk mem-break down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target.","title":"Manfaat Pohon Keputusan"},{"location":"Pohon Keputusan/#pohon-keputusan-mempunyai-3-tipe-simpul","text":"Simpul akar, dimana tidak memiliki cabang yang masuk dan memiliki cabang lebih dari satu, terkadang tidak memiliki cabang sama sekali. Simpul ini biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. Simpul internal, dimana hanya memiliki 1 cabang yang masuk, dan memiliki lebih dari 1 cabang yang keluar. Simpul daun, atau simpul akhir dimana hanya memiliki 1 cabang yang masuk, dan tidak memiliki cabang sama sekali dan menandai bahwa simpul tersebut merupakan label kelas.","title":"Pohon keputusan mempunyai 3 tipe simpul"},{"location":"Pohon Keputusan/#kelebihan-pohon-keputusan","text":"Tidak memerlukan biaya yang mahal saat membangun algoritma. Mudah untuk diinterpetasikan. Mudah mengintegrasikan dengan sistem basis data. Memiliki nilai ketelitian yang lebih baik. Dapat menemukan hubungan tak terduga dan suatu data. Dapat menggunakan data pasti/mutlak atau data kontinu. Mengakomodasi data yang hilang.","title":"Kelebihan Pohon Keputusan"},{"location":"Pohon Keputusan/#kekurangan-pohon-keputusan","text":"Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain.","title":"Kekurangan pohon keputusan"},{"location":"Pohon Keputusan/#cara-membuat-decision-tree","text":"Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen ( kolom , data ): kelas = [] for i in range ( len ( data )): if data . values . tolist ()[ i ][ kolom ] not in kelas : kelas . append ( data . values . tolist ()[ i ][ kolom ]) return kelas kelas = banyak_elemen ( df . shape [ 1 ] - 1 , df ) outlook = banyak_elemen ( df . shape [ 1 ] - 5 , df ) temp = banyak_elemen ( df . shape [ 1 ] - 4 , df ) humidity = banyak_elemen ( df . shape [ 1 ] - 3 , df ) windy = banyak_elemen ( df . shape [ 1 ] - 2 , df ) print ( kelas , outlook , temp , humidity , windy ) [ 'no' , 'yes' ] [ 'sunny' , 'overcast' , 'rainy' ] [ 'hot' , 'mild' , 'cool' ] [ 'high' , 'normal' ] [ False , True ] Fungsi count Kelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas ( kelas , kolomKelas , data ): hasil = [] for x in range ( len ( kelas )): hasil . append ( 0 ) for i in range ( len ( data )): for j in range ( len ( kelas )): if data . values . tolist ()[ i ][ kolomKelas ] == kelas [ j ]: hasil [ j ] += 1 return hasil pKelas = countvKelas ( kelas , df . shape [ 1 ] - 1 , df ) pKelas [ 5 , 9 ] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy ( T ): hasil = 0 jumlah = 0 for y in T : jumlah += y for z in range ( len ( T )): if jumlah != 0 : T [ z ] = T [ z ] / jumlah for i in T : if i != 0 : hasil -= i * math . log ( i , 2 ) return hasil def e_list ( atribut , n ): temp = [] tx = t_list ( atribut , n ) for i in range ( len ( atribut )): ent = entropy ( tx [ i ]) temp . append ( ent ) return temp tOutlook = t_list ( outlook , 5 ) tTemp = t_list ( temp , 4 ) tHum = t_list ( humidity , 3 ) tWin = t_list ( windy , 2 ) print ( \"Sunny, Overcast, Rainy\" , eOutlook ) print ( \"Hot, Mild, Cold\" , eTemp ) print ( \"High, Normal\" , eHum ) print ( \"False, True\" , eWin ) Sunny , Overcast , Rainy [ 0.9709505944546686 , 0.0 , 0.9709505944546686 ] Hot , Mild , Cold [ 1.0 , 0.9182958340544896 , 0.8112781244591328 ] High , Normal [ 0.9852281360342516 , 0.5916727785823275 ] False , True [ 0.8112781244591328 , 1.0 ] berikut contoh data yang akan di rubah menjadi decision tree 0 1 2 3 4 0 CUSTOMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUXURY EXTRA LARGE C1 15 15 F LUXURY SMALL C1 16 16 F LUXURY SMALL C1 17 17 F LUXURY MEDIUM C1 18 18 F LUXURY MEDIUM C1 19 19 F LUXURY MEDIUM C1 20 20 F LUXURY LARGE C1 pertama mencari entropy(s) dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"Cara Membuat Decision Tree"},{"location":"Statistik Dekriptif/","text":"Statistik Deskriptif \u00b6 Pengertian \u00b6 Statistik deskriptif adalah koefisien deskriptif singkat yang meringkas set data yang diberikan, yang dapat berupa representasi keseluruhan atau sampel populasi. Statistik deskriptif dipecah menjadi ukuran tendensi sentral dan ukuran variabilitas (sebaran). Ukuran tendensi sentral meliputi mean, median, dan mode, sedangkan ukuran variabilitas meliputi deviasi standar, varians, variabel minimum dan maksimum, dan kurtosis dan skewness. Tipe Statistik Deskriptif \u00b6 Mean (rata-rata) \u00b6 Mean merupakan rata-rata dari semua angka. Mean didapat dari penjumlahan keseluruhan angka dibagi dengan banyaknya angka itu sendiri. Jika kita memiliki N data, kita dapat menghitung mean itu sendiri dengan menggunakan rumus berikut : $$ \u00afx=n\u2211i=1xiN=x1+x2+x3+...+xnN $$ Dimana: x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyaknya data Median \u00b6 Median merupakan pusat data atau lebih sering dikatakan nilai tengah dari sebuah urutan data. Median disimbolkan dengan Me . nilai dari median akan sama dengan nilai Quartile 2 ( Q2 ). Untuk mencari median kita dapat menggunakan rumus sebagai berikut : $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Median dari kelompok data n = banyak data Modus \u00b6 Modus adalah angka yang paling sering ditemukan dalam suatu himpunan angka. Modus didapat dengan mengumpulkan dan mengatur data untuk menghitung setiap frekuensi dari setiap hasil. Hasil dengan jumlah tertinggi merupakan modus. Untuk mencari modus dari sebuah himpunan angka dapat menggunakan rumus berikut : $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak (selalu positif) Varians \u00b6 Varian adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Untuk menghitung varian terdapat formula yang dapat digunakan yaitu : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Dimana : xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data Standar Deviasi \u00b6 Standar deviasi merupakan ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Untuk mencari standar deviasi kita dapat menggunakan formula berikut : $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Skewness \u00b6 Skewness ( kemiringan ) adalah ketidaksimetrisan pada suatu distribusi statistik dimana kurva tampak condong ke kiri atau ke kanan. Skewness digunakan untuk menentukan sejauh mana perbedaan suatu distribusi dengan distribusi normal. Skewness bisa dihitung menggunakan rumus sebagai berikut : $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Dimana : xi = titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi Quartile \u00b6 Quartile adalah irisan nilai dari hasil pembagian data menjadi empat bagian yang sama besar. Yaitu Q1, Q2, Q3, dan Q4. Dalam mencari quatile kita dapat menggunakan rumus berikut ini : $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Dimana : Q = Nilai dari quartile n = banyak dari himpunan data Penerapan Statistik Deskriptif Menggunakan Python \u00b6 Alat dan Bahan \u00b6 Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. Dalam kasus ini, library python yang digunakan adalah pandas dan scipy. import pandas as pd from scipy import stats df = pd . read_csv ( 'sampel.csv' , sep = ';' ) data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standar Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'Stats' ] + [ x for x in df . columns ]) tes Berikut hasil gabungan dari code yang telah di buat untuk menampilkan program tabel dibawah ini : Stats 2016 2017 2018 2019 0 Min 20.000 30.000 10.000 15.000 1 Max 98.000 94.000 100.000 96.000 2 Mean 57.826 61.878 55.622 53.432 3 Standar Deviasi 22.280 18.700 25.910 22.920 4 Variasi 496.580 349.650 671.110 525.110 5 Skewnes 0.090 0.040 -0.070 0.110 6 Quantile 1 39.000 46.000 33.000 34.000 7 Quantile 2 58.000 62.000 58.500 54.000 8 Quantile 3 77.000 77.250 76.250 72.000 9 Median 58.000 62.000 58.500 54.000 10 Modus 65.000 33.000 77.000 31.000 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Statistik Deskriptif"},{"location":"Statistik Dekriptif/#statistik-deskriptif","text":"","title":"Statistik Deskriptif"},{"location":"Statistik Dekriptif/#pengertian","text":"Statistik deskriptif adalah koefisien deskriptif singkat yang meringkas set data yang diberikan, yang dapat berupa representasi keseluruhan atau sampel populasi. Statistik deskriptif dipecah menjadi ukuran tendensi sentral dan ukuran variabilitas (sebaran). Ukuran tendensi sentral meliputi mean, median, dan mode, sedangkan ukuran variabilitas meliputi deviasi standar, varians, variabel minimum dan maksimum, dan kurtosis dan skewness.","title":"Pengertian"},{"location":"Statistik Dekriptif/#tipe-statistik-deskriptif","text":"","title":"Tipe Statistik Deskriptif"},{"location":"Statistik Dekriptif/#mean-rata-rata","text":"Mean merupakan rata-rata dari semua angka. Mean didapat dari penjumlahan keseluruhan angka dibagi dengan banyaknya angka itu sendiri. Jika kita memiliki N data, kita dapat menghitung mean itu sendiri dengan menggunakan rumus berikut : $$ \u00afx=n\u2211i=1xiN=x1+x2+x3+...+xnN $$ Dimana: x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyaknya data","title":"Mean (rata-rata)"},{"location":"Statistik Dekriptif/#median","text":"Median merupakan pusat data atau lebih sering dikatakan nilai tengah dari sebuah urutan data. Median disimbolkan dengan Me . nilai dari median akan sama dengan nilai Quartile 2 ( Q2 ). Untuk mencari median kita dapat menggunakan rumus sebagai berikut : $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Median dari kelompok data n = banyak data","title":"Median"},{"location":"Statistik Dekriptif/#modus","text":"Modus adalah angka yang paling sering ditemukan dalam suatu himpunan angka. Modus didapat dengan mengumpulkan dan mengatur data untuk menghitung setiap frekuensi dari setiap hasil. Hasil dengan jumlah tertinggi merupakan modus. Untuk mencari modus dari sebuah himpunan angka dapat menggunakan rumus berikut : $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak (selalu positif)","title":"Modus"},{"location":"Statistik Dekriptif/#varians","text":"Varian adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Untuk menghitung varian terdapat formula yang dapat digunakan yaitu : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Dimana : xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data","title":"Varians"},{"location":"Statistik Dekriptif/#standar-deviasi","text":"Standar deviasi merupakan ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Untuk mencari standar deviasi kita dapat menggunakan formula berikut : $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$","title":"Standar Deviasi"},{"location":"Statistik Dekriptif/#skewness","text":"Skewness ( kemiringan ) adalah ketidaksimetrisan pada suatu distribusi statistik dimana kurva tampak condong ke kiri atau ke kanan. Skewness digunakan untuk menentukan sejauh mana perbedaan suatu distribusi dengan distribusi normal. Skewness bisa dihitung menggunakan rumus sebagai berikut : $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Dimana : xi = titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi","title":"Skewness"},{"location":"Statistik Dekriptif/#quartile","text":"Quartile adalah irisan nilai dari hasil pembagian data menjadi empat bagian yang sama besar. Yaitu Q1, Q2, Q3, dan Q4. Dalam mencari quatile kita dapat menggunakan rumus berikut ini : $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Dimana : Q = Nilai dari quartile n = banyak dari himpunan data","title":"Quartile"},{"location":"Statistik Dekriptif/#penerapan-statistik-deskriptif-menggunakan-python","text":"","title":"Penerapan Statistik Deskriptif Menggunakan Python"},{"location":"Statistik Dekriptif/#alat-dan-bahan","text":"Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. Dalam kasus ini, library python yang digunakan adalah pandas dan scipy. import pandas as pd from scipy import stats df = pd . read_csv ( 'sampel.csv' , sep = ';' ) data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standar Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'Stats' ] + [ x for x in df . columns ]) tes Berikut hasil gabungan dari code yang telah di buat untuk menampilkan program tabel dibawah ini : Stats 2016 2017 2018 2019 0 Min 20.000 30.000 10.000 15.000 1 Max 98.000 94.000 100.000 96.000 2 Mean 57.826 61.878 55.622 53.432 3 Standar Deviasi 22.280 18.700 25.910 22.920 4 Variasi 496.580 349.650 671.110 525.110 5 Skewnes 0.090 0.040 -0.070 0.110 6 Quantile 1 39.000 46.000 33.000 34.000 7 Quantile 2 58.000 62.000 58.500 54.000 8 Quantile 3 77.000 77.250 76.250 72.000 9 Median 58.000 62.000 58.500 54.000 10 Modus 65.000 33.000 77.000 31.000 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Alat dan Bahan"},{"location":"clustering/","text":"Clustering Categorical Data \u00b6 Clustering adalah sebuah proses untuk mengelompokan data ke dalam beberapa cluster atau kelompok sehingga data dalam satu cluster memiliki tingkat kemiripan yang maksimum dan data antar cluster memiliki kemiripan yang minimum. Metode K-Means Clustering \u00b6 K-Means adalah salah satu algoritma clustering / pengelompokan data yang bersifat Unsupervised Learning, yang berarti masukan dari algoritma ini menerima data tanpa label kelas. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. Karakteristik dari algoritma ini adalah : Memiliki n buah data Input berupa jumlah data dan jumlah cluster (kelompok) Pada setiap cluster / kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Algoritma K-Means \u00b6 Secara sederhana algoritma K-Means dimulai dari tahap berikut : Pilih K buah titik centroid. Menghitung jarak data dengan centroid. Update nilai titik centroid. Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah. Rumus K-Means \u00b6 Metode K-Modes \u00b6 K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster. Rumus K-Modes \u00b6 Metode K-Prototype \u00b6 Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu. Rumus K-Prototype \u00b6 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering"},{"location":"clustering/#clustering-categorical-data","text":"Clustering adalah sebuah proses untuk mengelompokan data ke dalam beberapa cluster atau kelompok sehingga data dalam satu cluster memiliki tingkat kemiripan yang maksimum dan data antar cluster memiliki kemiripan yang minimum.","title":"Clustering Categorical Data"},{"location":"clustering/#metode-k-means-clustering","text":"K-Means adalah salah satu algoritma clustering / pengelompokan data yang bersifat Unsupervised Learning, yang berarti masukan dari algoritma ini menerima data tanpa label kelas. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. Karakteristik dari algoritma ini adalah : Memiliki n buah data Input berupa jumlah data dan jumlah cluster (kelompok) Pada setiap cluster / kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut.","title":"Metode K-Means Clustering"},{"location":"clustering/#algoritma-k-means","text":"Secara sederhana algoritma K-Means dimulai dari tahap berikut : Pilih K buah titik centroid. Menghitung jarak data dengan centroid. Update nilai titik centroid. Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah.","title":"Algoritma K-Means"},{"location":"clustering/#rumus-k-means","text":"","title":"Rumus K-Means"},{"location":"clustering/#metode-k-modes","text":"K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster.","title":"Metode K-Modes"},{"location":"clustering/#rumus-k-modes","text":"","title":"Rumus K-Modes"},{"location":"clustering/#metode-k-prototype","text":"Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu.","title":"Metode K-Prototype"},{"location":"clustering/#rumus-k-prototype","text":"MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Rumus K-Prototype"},{"location":"fuzzy/","text":"Fuzzy C-Means \u00b6 Pengertian \u00b6 Fuzzy clustering adalah proses menentukan derajat keanggotaan, dan kemudian menggunakannya dengan memasukkannya kedalam elemen data kedalam satu kelompok cluster atau lebih. Hal ini akan memberikan informasi kesamaan dari setiap objek. Satu dari sekian banyaknya algoritma fuzzy clustering yang digunakan adalah algoritma fuzzy clustering c means. Vektor dari fuzzy clustering, V={v1, v2, v3,\u2026, vc}, merupakan sebuah fungsi objektif yang di defenisikan dengan derajat keanggotaan dari data Xj dan pusat cluster Vj. Algoritma fuzzy clustering c means membagi data yang tersedia dari setiap elemen data berhingga lalu memasukkannya kedalam bagian dari koleksi cluster yang dipengaruhi oleh beberapa kriteria yang diberikan. Berikan satu kumpulan data berhingga. X= {x1,\u2026, xn } dan pusat data. Dimana \u03bc ij adalah derajat keanggotaan dari Xj dan pusat cluster adalah sebuah bagian dari keanggotaan matriks [\u03bc ij]. d2 adalah akar dari Euclidean distance dan m adalah parameter fuzzy yang rata-rata derajat kekaburan dari setiap data derajat keanggotaan tidak lebih besar dari 1,0 Output dari Fuzzy C-Means merupakan deretan pusat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Algoritma Fuzzy C-Means \u00b6 Secara sederhana algoritma K-Means dimulai dari tahap berikut : Input data yang akan dicluster X, berupa matriks berukuran n x m (n=jumlah sample data, m=atribut setiap data). Xij=data sample ke-i (i=1,2,\u2026,n), atribut ke-j (j=1,2,\u2026,m). Tentukan : Jumlah cluster = c Pangkat = w Maksimum iterasi = MaxIter Error terkecil yang diharapkan = \u03be Fungsi obyektif awal = Po = 0 Iterasi awal = t Bangkitkan nilai acak \u03bcik, i=1,2,\u2026,n; k=1,2,\u2026,c sebagai elemen-elemen matriks partisi awal \u03bcik. \u03bcik adalah derajat keanggotaan yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster.Posisi dan nilai matriks dibangun secara random. Dimana nilai keangotaan terletak pada interval 0 sampai dengan 1. Pada posisi awal matriks partisi U masih belum akurat begitu juga pusat clusternya. Sehingga kecendrungan data untuk masuk suatu cluster juga belum akurat. Input data yang akan dicluster X, berupa matriks berukuran n x m (n=jumlah sample data, m=atribut setiap data). Xij=data sample ke-i (i=1,2,\u2026,n), atribut ke-j (j=1,2,\u2026,m). Fungsi objektif digunakan sebagai syarat perulangan untuk mendapatkan pusat cluster yang tepat. Sehingga diperoleh kecendrungan data untuk masuk ke *cluster* mana pada *step* akhir. Hitung fungsi obyektif pada iterasi ke-t, Pt Perhitungan fungsi objektif Pt dimana nilai variabel fuzzy Xij di kurang dengan dengan pusat cluster Vkj kemudian hasil pengurangannya di kuadradkan lalu masing-masing hasil kuadrad di jumlahkan untuk dikali dengan kuadrad dari derajat keanggotaan \u03bcik untuk tiap cluster. Setelah itu jumlahkan semua nilai di semua cluster untuk mendapatkan fungsi objektif Pt. Hitung perubahan matriks partisi: Cek kondisi berhenti:a) jika:( |Pt \u2013 Pt-1 | < \u03be) atau (t>maxIter) maka berhenti.b) jika tidak, t=t+1, ulangi langkah ke-4. Contoh Code Fuzzy C-Means \u00b6 from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"Fuzzy C-Means"},{"location":"fuzzy/#fuzzy-c-means","text":"","title":"Fuzzy C-Means"},{"location":"fuzzy/#pengertian","text":"Fuzzy clustering adalah proses menentukan derajat keanggotaan, dan kemudian menggunakannya dengan memasukkannya kedalam elemen data kedalam satu kelompok cluster atau lebih. Hal ini akan memberikan informasi kesamaan dari setiap objek. Satu dari sekian banyaknya algoritma fuzzy clustering yang digunakan adalah algoritma fuzzy clustering c means. Vektor dari fuzzy clustering, V={v1, v2, v3,\u2026, vc}, merupakan sebuah fungsi objektif yang di defenisikan dengan derajat keanggotaan dari data Xj dan pusat cluster Vj. Algoritma fuzzy clustering c means membagi data yang tersedia dari setiap elemen data berhingga lalu memasukkannya kedalam bagian dari koleksi cluster yang dipengaruhi oleh beberapa kriteria yang diberikan. Berikan satu kumpulan data berhingga. X= {x1,\u2026, xn } dan pusat data. Dimana \u03bc ij adalah derajat keanggotaan dari Xj dan pusat cluster adalah sebuah bagian dari keanggotaan matriks [\u03bc ij]. d2 adalah akar dari Euclidean distance dan m adalah parameter fuzzy yang rata-rata derajat kekaburan dari setiap data derajat keanggotaan tidak lebih besar dari 1,0 Output dari Fuzzy C-Means merupakan deretan pusat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system.","title":"Pengertian"},{"location":"fuzzy/#algoritma-fuzzy-c-means","text":"Secara sederhana algoritma K-Means dimulai dari tahap berikut : Input data yang akan dicluster X, berupa matriks berukuran n x m (n=jumlah sample data, m=atribut setiap data). Xij=data sample ke-i (i=1,2,\u2026,n), atribut ke-j (j=1,2,\u2026,m). Tentukan : Jumlah cluster = c Pangkat = w Maksimum iterasi = MaxIter Error terkecil yang diharapkan = \u03be Fungsi obyektif awal = Po = 0 Iterasi awal = t Bangkitkan nilai acak \u03bcik, i=1,2,\u2026,n; k=1,2,\u2026,c sebagai elemen-elemen matriks partisi awal \u03bcik. \u03bcik adalah derajat keanggotaan yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster.Posisi dan nilai matriks dibangun secara random. Dimana nilai keangotaan terletak pada interval 0 sampai dengan 1. Pada posisi awal matriks partisi U masih belum akurat begitu juga pusat clusternya. Sehingga kecendrungan data untuk masuk suatu cluster juga belum akurat. Input data yang akan dicluster X, berupa matriks berukuran n x m (n=jumlah sample data, m=atribut setiap data). Xij=data sample ke-i (i=1,2,\u2026,n), atribut ke-j (j=1,2,\u2026,m). Fungsi objektif digunakan sebagai syarat perulangan untuk mendapatkan pusat cluster yang tepat. Sehingga diperoleh kecendrungan data untuk masuk ke *cluster* mana pada *step* akhir. Hitung fungsi obyektif pada iterasi ke-t, Pt Perhitungan fungsi objektif Pt dimana nilai variabel fuzzy Xij di kurang dengan dengan pusat cluster Vkj kemudian hasil pengurangannya di kuadradkan lalu masing-masing hasil kuadrad di jumlahkan untuk dikali dengan kuadrad dari derajat keanggotaan \u03bcik untuk tiap cluster. Setelah itu jumlahkan semua nilai di semua cluster untuk mendapatkan fungsi objektif Pt. Hitung perubahan matriks partisi: Cek kondisi berhenti:a) jika:( |Pt \u2013 Pt-1 | < \u03be) atau (t>maxIter) maka berhenti.b) jika tidak, t=t+1, ulangi langkah ke-4.","title":"Algoritma Fuzzy C-Means"},{"location":"fuzzy/#contoh-code-fuzzy-c-means","text":"from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"Contoh Code Fuzzy C-Means"},{"location":"index2/","text":"Selamat Datang di Komputasi Numerik \u00b6 Nama : Anggrahito Cahyo Dwi Prasetyaningtyas NIM : 180411100019 Kelas : Komputasi Numerik 4B Dosen Pengampu : Mula'ab, S.Si., M.Kom Jurusan : Teknik Informatika","title":"Index"},{"location":"index2/#selamat-datang-di-komputasi-numerik","text":"Nama : Anggrahito Cahyo Dwi Prasetyaningtyas NIM : 180411100019 Kelas : Komputasi Numerik 4B Dosen Pengampu : Mula'ab, S.Si., M.Kom Jurusan : Teknik Informatika","title":"Selamat Datang di Komputasi Numerik"},{"location":"regresi/","text":"Regresi Linier Sederhana \u00b6 Pengertian \u00b6 Regresi Linear Sederhana adalah Metode Statistik yang berfungsi untuk menguji sejauh mana hubungan sebab akibat antara Variabel Faktor Penyebab (X) terhadap Variabel Akibatnya. Faktor Penyebab pada umumnya dilambangkan dengan X atau disebut juga dengan Predictor sedangkan Variabel Akibat dilambangkan dengan Y atau disebut juga dengan Response. Regresi Linear Sederhana atau sering disingkat dengan SLR (Simple Linear Regression) juga merupakan salah satu Metode Statistik yang dipergunakan dalam produksi untuk melakukan peramalan ataupun prediksi tentang karakteristik kualitas maupun Kuantitas. Model Persamaan Regresi Linear Sederhana adalah seperti berikut ini : Y = a + bX \u00b6 Dimana : Y = Variabel Response atau Variabel Akibat (Dependent) X = Variabel Predictor atau Variabel Faktor Penyebab (Independent) a = konstanta b = koefisien regresi (kemiringan); besaran Response yang ditimbulkan oleh Predictor. Nilai-nilai a dan b dapat dihitung dengan menggunakan Rumus dibawah ini : a = (\u03a3y) (\u03a3x\u00b2) \u2013 (\u03a3x) (\u03a3xy) n(\u03a3x\u00b2) \u2013 (\u03a3x)\u00b2 b = n(\u03a3xy) \u2013 (\u03a3x) (\u03a3y) n(\u03a3x\u00b2) \u2013 (\u03a3x)\u00b2 Algoritma Regresi Linier Sederhana \u00b6 Berikut ini adalah Langkah-langkah dalam melakukan Analisis Regresi Linear Sederhana : Tentukan Tujuan dari melakukan Analisis Regresi Linear Sederhana Identifikasikan Variabel Faktor Penyebab (Predictor) dan Variabel Akibat (Response) Lakukan Pengumpulan Data Hitung X\u00b2, Y\u00b2, XY dan total dari masing-masingnya Hitung a dan b berdasarkan rumus diatas. Buatkan Model Persamaan Regresi Linear Sederhana. Lakukan Prediksi atau Peramalan terhadap Variabel Faktor Penyebab atau Variabel Akibat. Regresi Linier Berganda \u00b6 Pengertian \u00b6 Analisis regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio. Model Persamaan Regresi Linear Sederhana adalah seperti berikut ini : Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn \u00b6 Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan) Contoh Kasus Dengan Penghitungan Manual \u00b6 Seorang Engineer ingin mempelajari Hubungan antara Suhu Ruangan dengan Jumlah Cacat yang diakibatkannya, sehingga dapat memprediksi atau meramalkan jumlah cacat produksi jika suhu ruangan tersebut tidak terkendali. Engineer tersebut kemudian mengambil data selama 30 hari terhadap rata-rata (mean) suhu ruangan dan Jumlah Cacat Produksi. Penyelesaiannya mengikuti Langkah-langkah dalam Analisis Regresi Linear Sederhana adalah sebagai berikut : Langkah 1 : Penentuan Tujuan \u00b6 Tujuan : Memprediksi Jumlah Cacat Produksi jika suhu ruangan tidak terkendali Langkah 2 : Identifikasikan Variabel Penyebab dan Akibat \u00b6 Varibel Faktor Penyebab (X) : Suhu Ruangan, Variabel Akibat (Y) : Jumlah Cacat Produksi Langkah 3 : Pengumpulan Data \u00b6 Berikut ini adalah data yang berhasil dikumpulkan selama 30 hari (berbentuk tabel) : Tanggal Rata-rata Suhu Ruangan Jumlah Cacat 1 24 10 2 22 5 3 21 6 4 20 3 5 22 6 6 19 4 7 20 5 8 23 9 9 24 11 10 25 13 11 21 7 12 20 4 13 20 6 14 19 3 15 25 12 16 27 13 17 28 16 18 25 12 19 26 14 20 24 12 21 27 16 22 23 9 23 24 13 24 23 11 25 22 7 26 21 5 27 26 12 28 25 11 29 26 13 30 27 14 Langkah 4 : Hitung X\u00b2, Y\u00b2, XY dan total dari masing-masingnya \u00b6 Berikut ini adalah tabel yang telah dilakukan perhitungan X\u00b2, Y\u00b2, XY dan totalnya : Tanggal Rata-rata Suhu Ruangan Jumlah Cacat X2 Y2 XY 1 24 10 576 100 240 2 22 5 484 25 110 3 21 6 441 36 126 4 20 3 400 9 60 5 22 6 484 36 132 6 19 4 361 16 76 7 20 5 400 25 100 8 23 9 529 81 207 9 24 11 576 121 264 10 25 13 625 169 325 11 21 7 441 49 147 12 20 4 400 16 80 13 20 6 400 36 120 14 19 3 361 9 57 15 25 12 625 144 300 16 27 13 729 169 351 17 28 16 784 256 448 18 25 12 625 144 300 19 26 14 676 196 364 20 24 12 576 144 288 21 27 16 729 256 432 22 23 9 529 81 207 23 24 13 576 169 312 24 23 11 529 121 253 25 22 7 484 49 154 26 21 5 441 25 105 27 26 12 676 144 312 28 25 11 625 121 275 29 26 13 676 169 338 30 27 14 729 196 378 Langkah 5 : Hitung a dan b berdasarkan rumus Regresi Linear Sederhana \u00b6 Menghitung Konstanta (a) : a = (\u03a3y) (\u03a3x\u00b2) \u2013 (\u03a3x) (\u03a3xy) n(\u03a3x\u00b2) \u2013 (\u03a3x)\u00b2 a = (282) (16.487) \u2013 (699) (6.861) 30 (16.487) \u2013 (699)\u00b2 a = -24,38 Menghitung Koefisien Regresi (b) b = n(\u03a3xy) \u2013 (\u03a3x) (\u03a3y) n(\u03a3x\u00b2) \u2013 (\u03a3x)\u00b2 b = 30 (6.861) \u2013 (699) (282) 30 (16.487) \u2013 (699)\u00b2 b = 1,45 Langkah 6 : Buat Model Persamaan Regresi \u00b6 Y = a + bX Y = -24,38 + 1,45X Langkah 7 : Lakukan Prediksi atau Peramalan terhadap Variabel Faktor Penyebab atau Variabel Akibat \u00b6 I. Prediksikan Jumlah Cacat Produksi jika suhu dalam keadaan tinggi (Variabel X), contohnya : 30\u00b0C Y = -24,38 + 1,45 (30) Y = 19,12 Jadi Jika Suhu ruangan mencapai 30\u00b0C, maka akan diprediksikan akan terdapat 19,12 unit cacat yang dihasilkan oleh produksi. II. Jika Cacat Produksi (Variabel Y) yang ditargetkan hanya boleh 4 unit, maka berapakah suhu ruangan yang diperlukan untuk mencapai target tersebut ? 4 = -24,38 + 1,45X 1,45X = 4 + 24,38 X = 28,38 / 1,45 X = 19,57 Jadi Prediksi Suhu Ruangan yang paling sesuai untuk mencapai target Cacat Produksi adalah sekitar 19,57\u00b0C Contoh Kasus Dengan Penghitungan Menggunakan Sklearn \u00b6 Berikut contoh data yang ingin dicari Langkah pertama adalah import librari yang ada pada python import numpy as np import pandas as pd import statsmodels import patsy import statsmodels.api as sm import matplotlib.pyplot as kemudian import model #Import model from sklearn.linear_model import LinearRegression from sklearn.cross_validation import train_test_split from sklearn import metrics Masukan data yang telah berbentuk csv dengan memanggil datanya sesuai tempat penyimpanannya python data=pd.read_csv(\u201cpost2.csv\u201d, sep=\u201d;\u201d) data Pilih HRG dan Kurs untuk variabel X dan EKS sebagai variabel y: feature_names = [ \u2018 HRG \u2019 , \u2019 KURS \u2019 ] X = data [ feature_names ] X y = data . EKS Kemudian memisahkan X dan y ke dalam data latih (train) dan data pengujian (test): X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 1 ) Linear Regression Model, Membuat model dengan data latih, Membuat prediksi pada data pengujian Linreg = LinearRegression () Linreg . fit ( X_train , y_train ) y_pred = linreg . predict ( X_test ) print ( np . sqrt ( metrics . mean_squared_error ( y_test , y_pred )) #mencari model regresi model = sm . OLS ( y , X ) . fit () predictions = model . predict ( X ) model . summary () #menambahkan variabel konstan X = sm . add_constant ( X ) model = sm . OLS ( y , X ) . fit () model . summary () Pada Gambar diatas diketahui bahwa fungsi yang dibuat menghasilkan model linier sederhana yaitu: Berdasarkan model diatas artinya jika HRG dan KURS mendekati nol maka nilai EKS -4067.4959. Sedangkan jika HRG naik satu satuan akan menaikkan nilai EKS sebesar 7.8150 dan jika KURS naik satu satuan maka akan menaikkan EKS sebesar 1001.8555 dengan R-square sebesar 0.911 atau 91.1% variabel HRG dan KURS dapat menjelaskan variabel EKS. - Uji parsial Uji parsial digunakan untuk menguji parameter secara parsial dengan kata lain untuk mengetahui apakah variable independent (X) berpengaruh secara signifikan (nyata) terhadap variable dependent (Y). Dari gambar didapat p-value (Konsatanta) sebesar 0.391, nilai (HRG) sebesar 0.001 dan nilai (KURS) sebesar 0.000. Berikut hipotesisnya: - Hipotesis \u200b H0 : \u03b2i = 0, i = 0,1,2 (Tidak terdapat pengaruh secara signifikan antara X dengan Y) \u200b H1 : \u03b2i \u2260 0, i = 0,1,2 (Ada pengaruh secara signifikan antara X dengan Y) - Tingkat signifikasi \u200b \u221d=5% = 0.05 - Daerah kritis \u200b Jika p- value \u2264 \u221d (0.05) \u2192 Tolak H0 - Statistik uji \u200b P-value* : = 0.001 dan = 0.000 ; \u221d= 0,05 - Keputusan \u200b Karena nilai p- value untuk \u03b21 dan \u03b22 < \u221d maka tolak - Kesimpulan \u200b Dengan tingkat kepercayaan 95% terdapat pengaruh secara signifikan antara variable X (HRG dan KURS) dengan variabel Y(EKS). - Uji Normalitas \u200b Uji Normalitaas adalah untuk melihat apakah nilai residual terdistribusi normal atau tidak. Model regresi yang baik adalah memiliki nilai residual yang terdistribusi normal. Disini kita menggunakan nilai prob Jaque Bera (JB) dari gambar output diatas sebesar 0.571. Dengan hipotesis sebagai berikut: \u200b - Menentukan Hipotesis \u200b H0 : Residual berdistribusi normal \u200b H1 : Residual tidak Berdistribusi Normal \u200b - Tingkat signifikansi \u200b \u221d=5% (\u221d=0.05) \u200b - Statistik Uji \u200b p-value = 0.571 \u200b - Daerah kritis \u200b Tolak H0 jika p-value < \u03b1 \u200b - Keputusan \u200b Karena nilai p-value sama dengan 0.571, dimana nilai p-value >\u03b1 yaitu 0.571 > 0,05 maka gagal tolak H0. \u200b - Kesimpulan \u200b Jadi, dengan tingkat kepercayaan 95% dapat disimpulkan bahwa residual berdistribusi normal. - Uji Autokorelasi \u200b Dianalisis autokorelasi ini jadi tidak boleh ada korelasi (non-autokorelasi) antara observasi dengan data observasi sebelumnya. Dapat dilihat dari gambar output diatas bahwa uji autokorelasi menggunakan output DW(Durbin-Watson) sebesar 2.162. \u200b - Menentukan Hipotesis \u200b H0 : Tidak terdapat Autokorelasi \u200b H1 : terdapat Autokorelasi \u200b - Tingkat signifikansi \u200b \u221d=5 (\u221d=0,05) \u200b - Menentukan daerah kritis \u200b Tolak H0 : jika 0 < DW < dl atau 4 \u2013 dl < DW < 4 \u200b Gagal tolak H0 : jika du < DW <4 \u2013 du \u200b Tidak ada keputusan : jika dl < DW < du atau 4 \u2013 du < DW < 4 \u2013 dl \u200b - Statistik uji \u200b Karena nilai DW 2.162 dngean du < DW <4 \u2013 du atau 0.9820 < DW <4 \u2013 1.5386 maka interval daerah keputusannya yaitu 0.9820< 2,162 < 2.4614 sehingga dapat dikatakan bahwa Tolak H0. \u200b - Kesimpulan \u200b Jadi, dengan tingkat signifikansi 5% dapat disimpulkan bahwa keputusan uji adalah Tidak terdapat Autokorelasi. - Uji Heteroskedastisitas \u200b Uji Heteroskedastistas untuk melihat apakah terdapat ketidaksamaan varians dari residual satu ke pengamatan yang lain. Disini antar pengamatan harus homoskedastisitas. Import statsmodels . formula . api as smf lm = smf . ols ( formula = \u201d EKS ~ HRG + KURS \u201d , data = data ) . fit () lm resid = lm . resid plt . scatter ( lm . predict (), resid ) import statsmodels.stats as stats stats . diagnostic . het_white ( resid , lm . model . exog , retres = False ) plt . show () Dari output diatas dapat diketahui bahwa titik-titik tidak berbentuk pula yang jelas. Dan titik-titik penyebar diatas dan dibawah angka 0 pada sumbu Y. Jadi dapat disimpulkan bahwa tidak terjadi masalah heteroskedastisitas dalam model regresi. - Uji Multikolinearitas \u200b Uji Multikolinearitas untuk melihat ada tidaknya korelasdi antara variabel bebas dalam regresi linear berganda. from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor lm = smf . ols ( formula = \u201c EKS ~ HRG + KURS \u201d , data = data ) . fit () y , X = dmatrices ( \u201c EKS ~ HRG + KURS \u201d , data = data , return_type = \u201d dataframe \u201d ) vif = [ variance_inflation_factor ( x , values , i ) for i in range ( X . shape [ 1 ])] print ( vif ) Menentukan Hipotesis H0 : VIF < 10 artinya tidak terdapat Multikolinearitas. H1 : VIF > 10 artinya terdapat Multikolinearitas. - Tingkat signifikansi \u200b \u221d=5% (\u221d=0.05) - Statistik Uji VIF : \u200b Konstan = 2.691 \u200b HRG = 1.248 \u200b KURS = 1.248 \u200b - Daerah Kritis \u200b Tolak H0 jika VIF > \u221d, Tolak H0 \u200b - Keputusan \u200b Karena nilai VIF (Konstan = 2.691, HRG =1.248, KURS = 1.248) < \u03b1 maka gagal Tolak H0 - Kesimpulan Jadi, dengan tingkat kepercayaan 95% dapat disimpulkan bahwa tidak terdapat Multikolinearitas.","title":"Regresi Linier"},{"location":"regresi/#regresi-linier-sederhana","text":"","title":"Regresi Linier Sederhana"},{"location":"regresi/#pengertian","text":"Regresi Linear Sederhana adalah Metode Statistik yang berfungsi untuk menguji sejauh mana hubungan sebab akibat antara Variabel Faktor Penyebab (X) terhadap Variabel Akibatnya. Faktor Penyebab pada umumnya dilambangkan dengan X atau disebut juga dengan Predictor sedangkan Variabel Akibat dilambangkan dengan Y atau disebut juga dengan Response. Regresi Linear Sederhana atau sering disingkat dengan SLR (Simple Linear Regression) juga merupakan salah satu Metode Statistik yang dipergunakan dalam produksi untuk melakukan peramalan ataupun prediksi tentang karakteristik kualitas maupun Kuantitas. Model Persamaan Regresi Linear Sederhana adalah seperti berikut ini :","title":"Pengertian"},{"location":"regresi/#y-a-bx","text":"Dimana : Y = Variabel Response atau Variabel Akibat (Dependent) X = Variabel Predictor atau Variabel Faktor Penyebab (Independent) a = konstanta b = koefisien regresi (kemiringan); besaran Response yang ditimbulkan oleh Predictor. Nilai-nilai a dan b dapat dihitung dengan menggunakan Rumus dibawah ini : a = (\u03a3y) (\u03a3x\u00b2) \u2013 (\u03a3x) (\u03a3xy) n(\u03a3x\u00b2) \u2013 (\u03a3x)\u00b2 b = n(\u03a3xy) \u2013 (\u03a3x) (\u03a3y) n(\u03a3x\u00b2) \u2013 (\u03a3x)\u00b2","title":"Y = a + bX"},{"location":"regresi/#algoritma-regresi-linier-sederhana","text":"Berikut ini adalah Langkah-langkah dalam melakukan Analisis Regresi Linear Sederhana : Tentukan Tujuan dari melakukan Analisis Regresi Linear Sederhana Identifikasikan Variabel Faktor Penyebab (Predictor) dan Variabel Akibat (Response) Lakukan Pengumpulan Data Hitung X\u00b2, Y\u00b2, XY dan total dari masing-masingnya Hitung a dan b berdasarkan rumus diatas. Buatkan Model Persamaan Regresi Linear Sederhana. Lakukan Prediksi atau Peramalan terhadap Variabel Faktor Penyebab atau Variabel Akibat.","title":"Algoritma Regresi Linier Sederhana"},{"location":"regresi/#regresi-linier-berganda","text":"","title":"Regresi Linier Berganda"},{"location":"regresi/#pengertian_1","text":"Analisis regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio. Model Persamaan Regresi Linear Sederhana adalah seperti berikut ini :","title":"Pengertian"},{"location":"regresi/#y-a-b1x1-b2x2-bnxn","text":"Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan)","title":"Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn"},{"location":"regresi/#contoh-kasus-dengan-penghitungan-manual","text":"Seorang Engineer ingin mempelajari Hubungan antara Suhu Ruangan dengan Jumlah Cacat yang diakibatkannya, sehingga dapat memprediksi atau meramalkan jumlah cacat produksi jika suhu ruangan tersebut tidak terkendali. Engineer tersebut kemudian mengambil data selama 30 hari terhadap rata-rata (mean) suhu ruangan dan Jumlah Cacat Produksi. Penyelesaiannya mengikuti Langkah-langkah dalam Analisis Regresi Linear Sederhana adalah sebagai berikut :","title":"Contoh Kasus Dengan Penghitungan Manual"},{"location":"regresi/#langkah-1-penentuan-tujuan","text":"Tujuan : Memprediksi Jumlah Cacat Produksi jika suhu ruangan tidak terkendali","title":"Langkah 1 : Penentuan Tujuan"},{"location":"regresi/#langkah-2-identifikasikan-variabel-penyebab-dan-akibat","text":"Varibel Faktor Penyebab (X) : Suhu Ruangan, Variabel Akibat (Y) : Jumlah Cacat Produksi","title":"Langkah 2 : Identifikasikan Variabel Penyebab dan Akibat"},{"location":"regresi/#langkah-3-pengumpulan-data","text":"Berikut ini adalah data yang berhasil dikumpulkan selama 30 hari (berbentuk tabel) : Tanggal Rata-rata Suhu Ruangan Jumlah Cacat 1 24 10 2 22 5 3 21 6 4 20 3 5 22 6 6 19 4 7 20 5 8 23 9 9 24 11 10 25 13 11 21 7 12 20 4 13 20 6 14 19 3 15 25 12 16 27 13 17 28 16 18 25 12 19 26 14 20 24 12 21 27 16 22 23 9 23 24 13 24 23 11 25 22 7 26 21 5 27 26 12 28 25 11 29 26 13 30 27 14","title":"Langkah 3 : Pengumpulan Data"},{"location":"regresi/#langkah-4-hitung-x2-y2-xy-dan-total-dari-masing-masingnya","text":"Berikut ini adalah tabel yang telah dilakukan perhitungan X\u00b2, Y\u00b2, XY dan totalnya : Tanggal Rata-rata Suhu Ruangan Jumlah Cacat X2 Y2 XY 1 24 10 576 100 240 2 22 5 484 25 110 3 21 6 441 36 126 4 20 3 400 9 60 5 22 6 484 36 132 6 19 4 361 16 76 7 20 5 400 25 100 8 23 9 529 81 207 9 24 11 576 121 264 10 25 13 625 169 325 11 21 7 441 49 147 12 20 4 400 16 80 13 20 6 400 36 120 14 19 3 361 9 57 15 25 12 625 144 300 16 27 13 729 169 351 17 28 16 784 256 448 18 25 12 625 144 300 19 26 14 676 196 364 20 24 12 576 144 288 21 27 16 729 256 432 22 23 9 529 81 207 23 24 13 576 169 312 24 23 11 529 121 253 25 22 7 484 49 154 26 21 5 441 25 105 27 26 12 676 144 312 28 25 11 625 121 275 29 26 13 676 169 338 30 27 14 729 196 378","title":"Langkah 4 : Hitung X\u00b2, Y\u00b2, XY dan total dari masing-masingnya"},{"location":"regresi/#langkah-5-hitung-a-dan-b-berdasarkan-rumus-regresi-linear-sederhana","text":"Menghitung Konstanta (a) : a = (\u03a3y) (\u03a3x\u00b2) \u2013 (\u03a3x) (\u03a3xy) n(\u03a3x\u00b2) \u2013 (\u03a3x)\u00b2 a = (282) (16.487) \u2013 (699) (6.861) 30 (16.487) \u2013 (699)\u00b2 a = -24,38 Menghitung Koefisien Regresi (b) b = n(\u03a3xy) \u2013 (\u03a3x) (\u03a3y) n(\u03a3x\u00b2) \u2013 (\u03a3x)\u00b2 b = 30 (6.861) \u2013 (699) (282) 30 (16.487) \u2013 (699)\u00b2 b = 1,45","title":"Langkah 5 : Hitung a dan b berdasarkan rumus Regresi Linear Sederhana"},{"location":"regresi/#langkah-6-buat-model-persamaan-regresi","text":"Y = a + bX Y = -24,38 + 1,45X","title":"Langkah 6 : Buat Model Persamaan Regresi"},{"location":"regresi/#langkah-7-lakukan-prediksi-atau-peramalan-terhadap-variabel-faktor-penyebab-atau-variabel-akibat","text":"I. Prediksikan Jumlah Cacat Produksi jika suhu dalam keadaan tinggi (Variabel X), contohnya : 30\u00b0C Y = -24,38 + 1,45 (30) Y = 19,12 Jadi Jika Suhu ruangan mencapai 30\u00b0C, maka akan diprediksikan akan terdapat 19,12 unit cacat yang dihasilkan oleh produksi. II. Jika Cacat Produksi (Variabel Y) yang ditargetkan hanya boleh 4 unit, maka berapakah suhu ruangan yang diperlukan untuk mencapai target tersebut ? 4 = -24,38 + 1,45X 1,45X = 4 + 24,38 X = 28,38 / 1,45 X = 19,57 Jadi Prediksi Suhu Ruangan yang paling sesuai untuk mencapai target Cacat Produksi adalah sekitar 19,57\u00b0C","title":"Langkah 7 : Lakukan Prediksi atau Peramalan terhadap Variabel Faktor Penyebab atau Variabel Akibat"},{"location":"regresi/#contoh-kasus-dengan-penghitungan-menggunakan-sklearn","text":"Berikut contoh data yang ingin dicari Langkah pertama adalah import librari yang ada pada python import numpy as np import pandas as pd import statsmodels import patsy import statsmodels.api as sm import matplotlib.pyplot as kemudian import model #Import model from sklearn.linear_model import LinearRegression from sklearn.cross_validation import train_test_split from sklearn import metrics Masukan data yang telah berbentuk csv dengan memanggil datanya sesuai tempat penyimpanannya python data=pd.read_csv(\u201cpost2.csv\u201d, sep=\u201d;\u201d) data Pilih HRG dan Kurs untuk variabel X dan EKS sebagai variabel y: feature_names = [ \u2018 HRG \u2019 , \u2019 KURS \u2019 ] X = data [ feature_names ] X y = data . EKS Kemudian memisahkan X dan y ke dalam data latih (train) dan data pengujian (test): X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 1 ) Linear Regression Model, Membuat model dengan data latih, Membuat prediksi pada data pengujian Linreg = LinearRegression () Linreg . fit ( X_train , y_train ) y_pred = linreg . predict ( X_test ) print ( np . sqrt ( metrics . mean_squared_error ( y_test , y_pred )) #mencari model regresi model = sm . OLS ( y , X ) . fit () predictions = model . predict ( X ) model . summary () #menambahkan variabel konstan X = sm . add_constant ( X ) model = sm . OLS ( y , X ) . fit () model . summary () Pada Gambar diatas diketahui bahwa fungsi yang dibuat menghasilkan model linier sederhana yaitu: Berdasarkan model diatas artinya jika HRG dan KURS mendekati nol maka nilai EKS -4067.4959. Sedangkan jika HRG naik satu satuan akan menaikkan nilai EKS sebesar 7.8150 dan jika KURS naik satu satuan maka akan menaikkan EKS sebesar 1001.8555 dengan R-square sebesar 0.911 atau 91.1% variabel HRG dan KURS dapat menjelaskan variabel EKS. - Uji parsial Uji parsial digunakan untuk menguji parameter secara parsial dengan kata lain untuk mengetahui apakah variable independent (X) berpengaruh secara signifikan (nyata) terhadap variable dependent (Y). Dari gambar didapat p-value (Konsatanta) sebesar 0.391, nilai (HRG) sebesar 0.001 dan nilai (KURS) sebesar 0.000. Berikut hipotesisnya: - Hipotesis \u200b H0 : \u03b2i = 0, i = 0,1,2 (Tidak terdapat pengaruh secara signifikan antara X dengan Y) \u200b H1 : \u03b2i \u2260 0, i = 0,1,2 (Ada pengaruh secara signifikan antara X dengan Y) - Tingkat signifikasi \u200b \u221d=5% = 0.05 - Daerah kritis \u200b Jika p- value \u2264 \u221d (0.05) \u2192 Tolak H0 - Statistik uji \u200b P-value* : = 0.001 dan = 0.000 ; \u221d= 0,05 - Keputusan \u200b Karena nilai p- value untuk \u03b21 dan \u03b22 < \u221d maka tolak - Kesimpulan \u200b Dengan tingkat kepercayaan 95% terdapat pengaruh secara signifikan antara variable X (HRG dan KURS) dengan variabel Y(EKS). - Uji Normalitas \u200b Uji Normalitaas adalah untuk melihat apakah nilai residual terdistribusi normal atau tidak. Model regresi yang baik adalah memiliki nilai residual yang terdistribusi normal. Disini kita menggunakan nilai prob Jaque Bera (JB) dari gambar output diatas sebesar 0.571. Dengan hipotesis sebagai berikut: \u200b - Menentukan Hipotesis \u200b H0 : Residual berdistribusi normal \u200b H1 : Residual tidak Berdistribusi Normal \u200b - Tingkat signifikansi \u200b \u221d=5% (\u221d=0.05) \u200b - Statistik Uji \u200b p-value = 0.571 \u200b - Daerah kritis \u200b Tolak H0 jika p-value < \u03b1 \u200b - Keputusan \u200b Karena nilai p-value sama dengan 0.571, dimana nilai p-value >\u03b1 yaitu 0.571 > 0,05 maka gagal tolak H0. \u200b - Kesimpulan \u200b Jadi, dengan tingkat kepercayaan 95% dapat disimpulkan bahwa residual berdistribusi normal. - Uji Autokorelasi \u200b Dianalisis autokorelasi ini jadi tidak boleh ada korelasi (non-autokorelasi) antara observasi dengan data observasi sebelumnya. Dapat dilihat dari gambar output diatas bahwa uji autokorelasi menggunakan output DW(Durbin-Watson) sebesar 2.162. \u200b - Menentukan Hipotesis \u200b H0 : Tidak terdapat Autokorelasi \u200b H1 : terdapat Autokorelasi \u200b - Tingkat signifikansi \u200b \u221d=5 (\u221d=0,05) \u200b - Menentukan daerah kritis \u200b Tolak H0 : jika 0 < DW < dl atau 4 \u2013 dl < DW < 4 \u200b Gagal tolak H0 : jika du < DW <4 \u2013 du \u200b Tidak ada keputusan : jika dl < DW < du atau 4 \u2013 du < DW < 4 \u2013 dl \u200b - Statistik uji \u200b Karena nilai DW 2.162 dngean du < DW <4 \u2013 du atau 0.9820 < DW <4 \u2013 1.5386 maka interval daerah keputusannya yaitu 0.9820< 2,162 < 2.4614 sehingga dapat dikatakan bahwa Tolak H0. \u200b - Kesimpulan \u200b Jadi, dengan tingkat signifikansi 5% dapat disimpulkan bahwa keputusan uji adalah Tidak terdapat Autokorelasi. - Uji Heteroskedastisitas \u200b Uji Heteroskedastistas untuk melihat apakah terdapat ketidaksamaan varians dari residual satu ke pengamatan yang lain. Disini antar pengamatan harus homoskedastisitas. Import statsmodels . formula . api as smf lm = smf . ols ( formula = \u201d EKS ~ HRG + KURS \u201d , data = data ) . fit () lm resid = lm . resid plt . scatter ( lm . predict (), resid ) import statsmodels.stats as stats stats . diagnostic . het_white ( resid , lm . model . exog , retres = False ) plt . show () Dari output diatas dapat diketahui bahwa titik-titik tidak berbentuk pula yang jelas. Dan titik-titik penyebar diatas dan dibawah angka 0 pada sumbu Y. Jadi dapat disimpulkan bahwa tidak terjadi masalah heteroskedastisitas dalam model regresi. - Uji Multikolinearitas \u200b Uji Multikolinearitas untuk melihat ada tidaknya korelasdi antara variabel bebas dalam regresi linear berganda. from patsy import dmatrices from statsmodels.stats.outliers_influence import variance_inflation_factor lm = smf . ols ( formula = \u201c EKS ~ HRG + KURS \u201d , data = data ) . fit () y , X = dmatrices ( \u201c EKS ~ HRG + KURS \u201d , data = data , return_type = \u201d dataframe \u201d ) vif = [ variance_inflation_factor ( x , values , i ) for i in range ( X . shape [ 1 ])] print ( vif ) Menentukan Hipotesis H0 : VIF < 10 artinya tidak terdapat Multikolinearitas. H1 : VIF > 10 artinya terdapat Multikolinearitas. - Tingkat signifikansi \u200b \u221d=5% (\u221d=0.05) - Statistik Uji VIF : \u200b Konstan = 2.691 \u200b HRG = 1.248 \u200b KURS = 1.248 \u200b - Daerah Kritis \u200b Tolak H0 jika VIF > \u221d, Tolak H0 \u200b - Keputusan \u200b Karena nilai VIF (Konstan = 2.691, HRG =1.248, KURS = 1.248) < \u03b1 maka gagal Tolak H0 - Kesimpulan Jadi, dengan tingkat kepercayaan 95% dapat disimpulkan bahwa tidak terdapat Multikolinearitas.","title":"Contoh Kasus Dengan Penghitungan Menggunakan Sklearn"}]}